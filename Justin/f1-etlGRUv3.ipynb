{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce53dfcd-d9cb-4b46-8b31-c54d5e6a56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d965371b-2bb8-43ac-995e-8afa513f6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09052b5-ae3b-48d8-9900-b3f66d300845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1 \n",
    "from fastf1 import get_session\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from f1_etl import (\n",
    "    DataConfig,\n",
    "    SessionConfig,\n",
    "    create_safety_car_dataset,\n",
    ")\n",
    "from f1_etl.train import (\n",
    "    ModelEvaluationSuite,\n",
    "    create_metadata_from_f1_dataset,\n",
    "    prepare_data_with_validation,\n",
    "    create_model_metadata,\n",
    "    train_and_validate_model,\n",
    "    evaluate_on_external_dataset,\n",
    "    compare_performance_across_datasets,\n",
    ")\n",
    "\n",
    "fastf1.Cache.enable_cache('E:\\School Stuff\\F1cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f6f5172-bc5e-466d-ae09-9792dfa3b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 17:23:17,175 - f1_etl - INFO - Preprocessing configuration:\n",
      "2025-07-08 17:23:17,176 - f1_etl - INFO -   Missing values: enabled (forward_fill)\n",
      "2025-07-08 17:23:17,177 - f1_etl - INFO -   Normalization: enabled (standard)\n",
      "2025-07-08 17:23:17,177 - f1_etl - INFO -   Resampling: smote\n",
      "2025-07-08 17:23:17,178 - f1_etl - INFO - Driver configuration:\n",
      "2025-07-08 17:23:17,179 - f1_etl - INFO -   Global drivers: 168155463144222310143771822431112720\n",
      "2025-07-08 17:23:17,179 - f1_etl - INFO -   Qatar Grand Prix: 168155463144222310143771822431112720\n",
      "2025-07-08 17:23:17,179 - f1_etl - INFO -   Chinese Grand Prix: 168155463144222310143771822431112720\n",
      "2025-07-08 17:23:17,180 - f1_etl - INFO -   Mexico City Grand Prix: 168155463144222310143771822431112720\n",
      "2025-07-08 17:23:17,180 - f1_etl - INFO -   SÃ£o Paulo Grand Prix: 168155463144222310143771822431112720\n",
      "2025-07-08 17:23:17,181 - f1_etl - INFO -   Miami Grand Prix: 168155463144222310143771822431112720\n",
      "2025-07-08 17:23:17,181 - f1_etl - INFO -   United States Grand Prix: 168155463144222310143771822431112720\n",
      "2025-07-08 17:23:17,182 - f1_etl - INFO -   Monaco Grand Prix: 168155463144222310143771822431112720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 Qatar Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "events      WARNING \tCorrecting user input 'Qatar Grand Prix' to 'Qatar Grand Prix'\n",
      "core           INFO \tLoading data for Qatar Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '16', '81', '63', '10', '55', '14', '24', '20', '4', '77', '44', '22', '30', '23', '27', '11', '18', '43', '31']\n",
      "core           INFO \tLoading data for Chinese Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 Chinese Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core        WARNING \tDriver 1 completed the race distance 00:08.313000 before the recorded end of the session.\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '4', '11', '16', '55', '63', '14', '81', '44', '27', '31', '23', '10', '24', '18', '20', '2', '3', '22', '77']\n",
      "core           INFO \tLoading data for Mexico City Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 Mexico City Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['55', '4', '16', '44', '63', '1', '20', '81', '27', '10', '18', '43', '31', '77', '24', '30', '11', '14', '23', '22']\n",
      "core           INFO \tLoading data for SÃ£o Paulo Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 SÃ£o Paulo Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core        WARNING \tNo lap data for driver 23\n",
      "core        WARNING \tFailed to perform lap accuracy check - all laps marked as inaccurate (driver 23)\n",
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '31', '10', '63', '16', '4', '22', '81', '30', '44', '11', '50', '77', '14', '24', '55', '43', '23', '18', '27']\n",
      "core           INFO \tLoading data for Miami Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 Miami Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['4', '1', '16', '11', '55', '44', '22', '63', '14', '31', '27', '10', '81', '24', '3', '77', '18', '23', '20', '2']\n",
      "events      WARNING \tCorrecting user input 'United States Grand Prix' to 'United States Grand Prix'\n",
      "core           INFO \tLoading data for United States Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 United States Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['16', '55', '1', '4', '81', '63', '11', '27', '30', '43', '20', '10', '14', '22', '18', '23', '77', '31', '24', '44']\n",
      "core           INFO \tLoading data for Monaco Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 Monaco Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['16', '81', '55', '4', '63', '1', '44', '22', '23', '10', '14', '3', '77', '18', '2', '24', '31', '11', '27', '20']\n",
      "2025-07-08 17:24:16,157 - f1_etl - INFO - Creating new fixed vocabulary encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Track Status Analysis (training_data):\n",
      "   green       : 8530965 samples ( 84.4%)\n",
      "   red         : 513060 samples (  5.1%)\n",
      "   safety_car  : 654091 samples (  6.5%)\n",
      "   vsc         : 66622 samples (  0.7%)\n",
      "   vsc_ending  :  3448 samples (  0.0%)\n",
      "   yellow      : 345367 samples (  3.4%)\n",
      "   Missing classes: ['unknown']\n",
      "âœ… FixedVocabTrackStatusEncoder fitted\n",
      "   Classes seen: ['green', 'red', 'safety_car', 'vsc', 'vsc_ending', 'yellow']\n",
      "   Total classes: 7\n",
      "   Output mode: integer labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 17:24:21,100 - f1_etl - INFO - Original class distribution before resampling: {'green': 8530965, 'red': 513060, 'safety_car': 654091, 'unknown': 0, 'vsc': 66622, 'vsc_ending': 3448, 'yellow': 345367}\n",
      "2025-07-08 17:24:21,101 - f1_etl - INFO - Applying smote resampling at session/driver level\n",
      "2025-07-08 17:24:21,102 - f1_etl - INFO - Sampling strategy: minority\n",
      "2025-07-08 17:24:57,379 - f1_etl - INFO - Resampling complete: 10113553 -> 18456642 samples\n",
      "2025-07-08 17:24:57,380 - f1_etl - INFO - Class distribution before resampling:\n",
      "2025-07-08 17:24:57,381 - f1_etl - INFO -   1: 8530965\n",
      "2025-07-08 17:24:57,381 - f1_etl - INFO -   4: 654091\n",
      "2025-07-08 17:24:57,382 - f1_etl - INFO -   5: 513060\n",
      "2025-07-08 17:24:57,382 - f1_etl - INFO -   2: 345367\n",
      "2025-07-08 17:24:57,382 - f1_etl - INFO -   6: 66622\n",
      "2025-07-08 17:24:57,383 - f1_etl - INFO -   7: 3448\n",
      "2025-07-08 17:24:57,383 - f1_etl - INFO - Class distribution after resampling:\n",
      "2025-07-08 17:24:57,384 - f1_etl - INFO -   1: 8530965\n",
      "2025-07-08 17:24:57,384 - f1_etl - INFO -   4: 4115950\n",
      "2025-07-08 17:24:57,386 - f1_etl - INFO -   7: 2588237\n",
      "2025-07-08 17:24:57,386 - f1_etl - INFO -   6: 2363063\n",
      "2025-07-08 17:24:57,387 - f1_etl - INFO -   5: 513060\n",
      "2025-07-08 17:24:57,388 - f1_etl - INFO -   2: 345367\n",
      "2025-07-08 17:24:58,247 - f1_etl - INFO - Processing 18456642 total telemetry rows\n",
      "2025-07-08 17:24:58,248 - f1_etl - INFO - Grouping by: ['SessionId', 'Driver']\n",
      "2025-07-08 17:32:31,649 - f1_etl - INFO - Total sequences generated: 738157\n",
      "2025-07-08 17:32:33,168 - f1_etl - INFO - Generated 738157 sequences with shape (738157, 50, 9)\n",
      "2025-07-08 17:32:33,414 - f1_etl - INFO - Applying missing value imputation with strategy: forward_fill\n",
      "2025-07-08 17:32:33,649 - f1_etl - INFO - Handling missing values with strategy: forward_fill\n",
      "2025-07-08 17:32:46,258 - f1_etl - INFO - Applying normalization with method: standard\n",
      "2025-07-08 17:32:52,217 - f1_etl - INFO - Final dataset summary:\n",
      "2025-07-08 17:32:52,218 - f1_etl - INFO -   Sequences: 738157\n",
      "2025-07-08 17:32:52,218 - f1_etl - INFO -   Features: 9\n",
      "2025-07-08 17:32:52,219 - f1_etl - INFO -   Classes: 7 (integer)\n",
      "2025-07-08 17:32:52,219 - f1_etl - INFO -   Label shape: (738157,)\n",
      "2025-07-08 17:32:52,219 - f1_etl - INFO -     green       : 340771 samples ( 46.2%)\n",
      "2025-07-08 17:32:52,221 - f1_etl - INFO -     red         : 20629 samples (  2.8%)\n",
      "2025-07-08 17:32:52,221 - f1_etl - INFO -     safety_car  : 164801 samples ( 22.3%)\n",
      "2025-07-08 17:32:52,222 - f1_etl - INFO -     vsc         : 94684 samples ( 12.8%)\n",
      "2025-07-08 17:32:52,222 - f1_etl - INFO -     vsc_ending  : 103495 samples ( 14.0%)\n",
      "2025-07-08 17:32:52,222 - f1_etl - INFO -     yellow      : 13777 samples (  1.9%)\n"
     ]
    }
   ],
   "source": [
    "drivers=('16' '81' '55' '4' '63' '1' '44' '22' '23' '10' '14' '3' '77' '18' '2' '24' '31' '11' '27' '20')\n",
    "\n",
    "data_config = DataConfig(\n",
    "        sessions=[\n",
    "            SessionConfig(2024, \"Qatar Grand Prix\", \"R\"),\n",
    "            SessionConfig(2024, \"Chinese Grand Prix\", \"R\"),\n",
    "            SessionConfig(2024, \"Mexico City Grand Prix\", \"R\"),\n",
    "            SessionConfig(2024, \"SÃ£o Paulo Grand Prix\", \"R\"),\n",
    "            SessionConfig(2024, \"Miami Grand Prix\", \"R\"),\n",
    "            SessionConfig(2024, \"United States Grand Prix\", \"R\"),\n",
    "            SessionConfig(2024, \"Monaco Grand Prix\", \"R\"),\n",
    "        ],\n",
    "        drivers=drivers,\n",
    "        include_weather=False,\n",
    "    )\n",
    "\n",
    "dataset = create_safety_car_dataset(\n",
    "    config=data_config,\n",
    "    window_size=50,\n",
    "    prediction_horizon=100,\n",
    "    normalize=True,\n",
    "    target_column=\"TrackStatus\",\n",
    "    resampling_strategy=\"smote\",\n",
    ")\n",
    "\n",
    "# 2. Create metadata\n",
    "dataset_metadata = create_metadata_from_f1_dataset(\n",
    "    data_config=data_config,\n",
    "    dataset=dataset,\n",
    "    features_used=\"multivariate_all_9_features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94ed500f-0a23-4b35-8b59-511afacfd075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516709, 9, 50)\n"
     ]
    }
   ],
   "source": [
    "print(splits['X_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9383b43-47e7-4aad-a766-2358ed4b73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA SPLIT SUMMARY ===\n",
      "Total samples: 738,157\n",
      "Train: 516,709 (70.0%)\n",
      "Val:   None (skipped)\n",
      "Test:  221,348 (30.0%) - removed 100 samples\n",
      "\n",
      "Train class distribution:\n",
      "  Class 0: 240,881 (46.6%)\n",
      "  Class 1: 14,547 (2.8%)\n",
      "  Class 2: 120,280 (23.3%)\n",
      "  Class 4: 68,628 (13.3%)\n",
      "  Class 5: 63,574 (12.3%)\n",
      "  Class 6: 8,799 (1.7%)\n",
      "\n",
      "Test class distribution:\n",
      "  Class 0: 99,825 (45.1%)\n",
      "  Class 1: 6,082 (2.7%)\n",
      "  Class 2: 44,521 (20.1%)\n",
      "  Class 4: 26,021 (11.8%)\n",
      "  Class 5: 39,921 (18.0%)\n",
      "  Class 6: 4,978 (2.2%)\n"
     ]
    }
   ],
   "source": [
    "# 3. Prepare data\n",
    "splits = prepare_data_with_validation(dataset, val_size=0.0, test_size=0.3)\n",
    "class_names = list(dataset[\"label_encoder\"].class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa3f559e-cc1a-4bd9-946e-bb84cc19a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(splits['y_train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef13f6ba-7942-4a86-b243-71390c78ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING WITH TEST: gru_driver_1_6_8_1_5_5_4_6_3_1_4_4_2_2_2_3_1_0_1_4_3_7_7_1_8_2_2_4_3_1_1_1_2_7_2_0\n",
      "================================================================================\n",
      "Training on train set...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 9, got 50",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 118\u001b[0m\n\u001b[0;32m    108\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluationSuite(\n\u001b[0;32m    109\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    110\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    113\u001b[0m model_metadata \u001b[38;5;241m=\u001b[39m create_model_metadata(\n\u001b[0;32m    114\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    115\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    116\u001b[0m )\n\u001b[1;32m--> 118\u001b[0m training_results \u001b[38;5;241m=\u001b[39m train_and_validate_model(\n\u001b[0;32m    119\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    120\u001b[0m     splits\u001b[38;5;241m=\u001b[39msplits,\n\u001b[0;32m    121\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mclass_names,\n\u001b[0;32m    122\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[0;32m    123\u001b[0m     dataset_metadata\u001b[38;5;241m=\u001b[39mdataset_metadata,\n\u001b[0;32m    124\u001b[0m     model_metadata\u001b[38;5;241m=\u001b[39mmodel_metadata,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# 6. Evaluate on external dataset\u001b[39;00m\n\u001b[0;32m    128\u001b[0m external_config \u001b[38;5;241m=\u001b[39m DataConfig(\n\u001b[0;32m    129\u001b[0m     sessions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    130\u001b[0m         SessionConfig(\u001b[38;5;241m2024\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanadian Grand Prix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     include_weather\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    135\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\f1_etl\\train\\training.py:54\u001b[0m, in \u001b[0;36mtrain_and_validate_model\u001b[1;34m(model, splits, class_names, evaluator, dataset_metadata, model_metadata, validate_during_training)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on train set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(splits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m\"\u001b[39m], splits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     56\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set if requested and available\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m, in \u001b[0;36mPyTorchGRUWrapper.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     49\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X_batch)\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36mGRUClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(x)\n\u001b[0;32m     10\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# take last time step\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1390\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1385\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1386\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m   1387\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m-> 1390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1394\u001b[0m         hx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m   1402\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:361\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]\n\u001b[0;32m    360\u001b[0m ):\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m    362\u001b[0m     expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 9, got 50"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from datetime import datetime\n",
    "\n",
    "class PyTorchGRUWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_dim=96, num_layers=2, output_dim=1, \n",
    "                 dropout=0.2, lr=0.001, weight_decay=1e-05, epochs=30, batch_size=64):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Create model\n",
    "        self.model = GRUClassifier(\n",
    "            self.input_dim, self.hidden_dim, self.num_layers, self.output_dim, self.dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Convert to PyTorch tensors and create DataLoader\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Calculate class weights\n",
    "        pos_count = np.sum(y)\n",
    "        neg_count = len(y) - pos_count\n",
    "        pos_weight = torch.tensor([neg_count / pos_count]).to(self.device)\n",
    "        \n",
    "        # Setup training\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        # Training loop\n",
    "        best_loss = float('inf')\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = criterion(outputs, y_batch.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = total_loss / num_batches\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "        \n",
    "        return (predictions > 0.5).astype(int).flatten()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "        \n",
    "        # Return probabilities for both classes [negative_class, positive_class]\n",
    "        return np.column_stack([1 - probs, probs])\n",
    "\n",
    "\n",
    "\n",
    "# 4. Create sklearn-compatible model\n",
    "model_name = f'gru_driver_{\"_\".join(drivers)}'  # Fixed syntax\n",
    "model = PyTorchGRUWrapper(\n",
    "    input_dim=dataset['X'].shape[2],\n",
    "    hidden_dim=96,\n",
    "    num_layers=2,\n",
    "    epochs=30,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# 5. Your existing evaluation code\n",
    "run_id = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{model_name}\"\n",
    "\n",
    "evaluator = ModelEvaluationSuite(\n",
    "    output_dir=\"evaluation_results\",\n",
    "    run_id=run_id,\n",
    ")\n",
    "\n",
    "model_metadata = create_model_metadata(\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "training_results = train_and_validate_model(\n",
    "    model=model,\n",
    "    splits=splits,\n",
    "    class_names=class_names,\n",
    "    evaluator=evaluator,\n",
    "    dataset_metadata=dataset_metadata,\n",
    "    model_metadata=model_metadata,\n",
    ")\n",
    "\n",
    "# 6. Evaluate on external dataset\n",
    "external_config = DataConfig(\n",
    "    sessions=[\n",
    "        SessionConfig(2024, \"Canadian Grand Prix\", \"R\"),\n",
    "        SessionConfig(2024, \"Saudi Arabian Grand Prix\", \"R\"),\n",
    "    ],\n",
    "    drivers=drivers,\n",
    "    include_weather=False,\n",
    ")\n",
    "\n",
    "external_results = evaluate_on_external_dataset(\n",
    "    trained_model=training_results[\"model\"],\n",
    "    external_config=external_config,\n",
    "    original_dataset_metadata=dataset_metadata,\n",
    "    model_metadata=model_metadata,\n",
    "    class_names=class_names,\n",
    "    evaluator=evaluator,\n",
    "    resampling_strategy=dataset_metadata.resampling_strategy,\n",
    "    resampling_config=dataset_metadata.resampling_config,\n",
    ")\n",
    "\n",
    "# 7. Compare results\n",
    "compare_performance_across_datasets(training_results, external_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b66da6a-df2f-427c-aee5-d8ef6b76f7f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m compare_performance_across_datasets(training_results, external_results)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_results' is not defined"
     ]
    }
   ],
   "source": [
    "compare_performance_across_datasets(training_results, external_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0507f3-b8ce-4794-907e-098ffbf806a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
