{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3531c74-819b-4eb7-90fd-6cc5138e176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66427c01-f3bf-4276-a3dd-67863a7b654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303d50a5-f5c2-41c8-924d-9bead63255d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1 \n",
    "from fastf1 import get_session\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from f1_etl import SessionConfig, DataConfig, create_safety_car_dataset\n",
    "\n",
    "fastf1.Cache.enable_cache('E:\\School Stuff\\F1cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62adc81f-8773-4e76-af65-65e8edf95d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2022 Sao Paolo Grand Prix R\n",
      "Loading from cache: E:\\School Stuff\\F1cache\\2022_Sao_Paolo_Grand_Prix_R.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 22:13:02,210 - f1_etl - INFO - Processing 1512260 total telemetry rows\n",
      "2025-06-29 22:13:02,211 - f1_etl - INFO - Grouping by: ['SessionId', 'Driver']\n",
      "2025-06-29 22:13:21,735 - f1_etl - INFO - Total sequences generated: 30220\n",
      "2025-06-29 22:13:21,825 - f1_etl - INFO - Generated 30220 sequences with shape (30220, 100, 9)\n",
      "2025-06-29 22:13:21,843 - f1_etl - INFO - Handling missing values with strategy: forward_fill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 30220 sequences\n",
      "Features: ['Speed', 'RPM', 'nGear', 'Throttle', 'Brake', 'X', 'Y', 'Distance', 'DifferentialDistance']\n",
      "Class distribution: {'green': 25860, 'safety_car': 3620, 'vsc': 460, 'yellow': 280}\n"
     ]
    }
   ],
   "source": [
    "# Define a single race session\n",
    "session = SessionConfig(\n",
    "    year=2022,\n",
    "    race=\"Sao Paolo Grand Prix\",\n",
    "    session_type=\"R\"  # Race\n",
    ")\n",
    "\n",
    "# Configure the dataset\n",
    "config = DataConfig(\n",
    "    sessions=[session],\n",
    "    cache_dir=\"E:\\School Stuff\\F1cache\"\n",
    ")\n",
    "\n",
    "# Generate the dataset\n",
    "dataset_22 = create_safety_car_dataset(\n",
    "    config=config,\n",
    "    window_size=100,\n",
    "    prediction_horizon=10\n",
    ")\n",
    "\n",
    "print(f\"Generated {dataset_22['config']['n_sequences']} sequences\")\n",
    "print(f\"Features: {dataset_22['config']['feature_names']}\")\n",
    "print(f\"Class distribution: {dataset_22['class_distribution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee1170a-28db-44a2-825b-fe4406dbe376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binary label distribution:\n",
      "  Class 0 (negative): 26600 samples\n",
      "  Class 1 (positive): 3620 samples\n"
     ]
    }
   ],
   "source": [
    "y_2022 = (dataset_22['y'] == 1).astype(np.float32)\n",
    "print(\"\\nBinary label distribution:\")\n",
    "print(f\"  Class 0 (negative): {np.sum(y_2022 == 0)} samples\")\n",
    "print(f\"  Class 1 (positive): {np.sum(y_2022 == 1)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23867781-9d0d-488b-984f-f111b2d5ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2023 Sao Paolo Grand Prix R\n",
      "Loading from cache: E:\\School Stuff\\F1cache\\2023_Sao_Paolo_Grand_Prix_R.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 22:13:32,874 - f1_etl - INFO - Processing 1667860 total telemetry rows\n",
      "2025-06-29 22:13:32,874 - f1_etl - INFO - Grouping by: ['SessionId', 'Driver']\n",
      "2025-06-29 22:13:54,364 - f1_etl - INFO - Total sequences generated: 33320\n",
      "2025-06-29 22:13:54,462 - f1_etl - INFO - Generated 33320 sequences with shape (33320, 100, 9)\n",
      "2025-06-29 22:13:54,482 - f1_etl - INFO - No missing values detected, skipping imputation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 33320 sequences\n",
      "Features: ['Speed', 'RPM', 'nGear', 'Throttle', 'Brake', 'X', 'Y', 'Distance', 'DifferentialDistance']\n",
      "Class distribution: {'green': 28720, 'red': 3860, 'safety_car': 500, 'yellow': 240}\n"
     ]
    }
   ],
   "source": [
    "# Define a single race session\n",
    "session = SessionConfig(\n",
    "    year=2023,\n",
    "    race=\"Sao Paolo Grand Prix\",\n",
    "    session_type=\"R\"  # Race\n",
    ")\n",
    "\n",
    "# Configure the dataset\n",
    "config = DataConfig(\n",
    "    sessions=[session],\n",
    "    cache_dir=\"E:\\School Stuff\\F1cache\"\n",
    ")\n",
    "\n",
    "# Generate the dataset\n",
    "dataset_23 = create_safety_car_dataset(\n",
    "    config=config,\n",
    "    window_size=100,\n",
    "    prediction_horizon=10\n",
    ")\n",
    "\n",
    "print(f\"Generated {dataset_23['config']['n_sequences']} sequences\")\n",
    "print(f\"Features: {dataset_23['config']['feature_names']}\")\n",
    "print(f\"Class distribution: {dataset_23['class_distribution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dde5bc0-36ad-487c-a620-e19ffea0e685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binary label distribution:\n",
      "  Class 0 (negative): 32820 samples\n",
      "  Class 1 (positive): 500 samples\n"
     ]
    }
   ],
   "source": [
    "y_2023 = (dataset_23['y'] == 2).astype(np.float32)\n",
    "print(\"\\nBinary label distribution:\")\n",
    "print(f\"  Class 0 (negative): {np.sum(y_2023 == 0)} samples\")\n",
    "print(f\"  Class 1 (positive): {np.sum(y_2023 == 1)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367204be-19fe-4e88-b900-2a1775ed2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import (\n",
    "    # Core metrics for binary classification\n",
    "    precision_score,\n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    \n",
    "    # Confusion matrix\n",
    "    confusion_matrix,\n",
    "    \n",
    "    # Threshold optimization\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    \n",
    "    # Comprehensive report\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Define search spaces\n",
    "HYPERPARAMETER_GRID = {\n",
    "    # Model architecture\n",
    "    'hidden_dim': [32, 64, 128, 256],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.5],\n",
    "    \n",
    "    # Training parameters\n",
    "    'learning_rate': [1e-4, 5e-4, 1e-3, 5e-3],\n",
    "    'batch_size': [64, 128, 256],\n",
    "    'weight_decay': [0, 1e-5, 1e-4, 1e-3],\n",
    "    \n",
    "    # Data parameters\n",
    "    'sequence_length': [25, 50, 100, 150],\n",
    "    'prediction_horizon': [5, 10, 15, 20],\n",
    "    \n",
    "    # Class balancing\n",
    "    'pos_weight_multiplier': [0.5, 1.0, 2.0, 5.0],  # Multiply calculated pos_weight\n",
    "    \n",
    "    # Regularization\n",
    "    'gradient_clip': [None, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "# For quick testing - smaller grid\n",
    "QUICK_GRID = {\n",
    "    'hidden_dim': [32, 64],\n",
    "    'num_layers': [1, 2],\n",
    "    'learning_rate': [1e-4, 1e-3],\n",
    "    'batch_size': [64, 128],\n",
    "    'dropout': [0.2, 0.3]\n",
    "}\n",
    "\n",
    "MEDIUM_GRID = {\n",
    "    'hidden_dim': [64, 96],     # Expand best architecture\n",
    "    'num_layers': [2],\n",
    "    'learning_rate': [1e-3, 5e-3],  # Full LR range\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'batch_size': [64, 128],              # Keep best batch sizes\n",
    "    'weight_decay': [0, 1e-5, 1e-4]      # Light regularization focus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3145593-9d01-42fe-84ee-cec4dbdc545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.sigmoid = nn.Sigmoid()  # For binary classification ##NEED TO REMOVE\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]  # take last time step\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd10e0fa-7a06-4e8f-88ae-aab287ef9b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter search with 30 combinations...\n",
      "\n",
      "Trial 1/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.299, P=0.364, R=0.250\n",
      "\n",
      "Trial 2/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.441, P=0.516, R=0.382\n",
      "\n",
      "Trial 3/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.444, P=0.387, R=0.516\n",
      "\n",
      "Trial 4/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.165, P=0.109, R=0.324\n",
      "\n",
      "Trial 5/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.438, P=0.459, R=0.416\n",
      "\n",
      "Trial 6/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.403, P=0.366, R=0.444\n",
      "\n",
      "Trial 7/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 96, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.445, P=0.405, R=0.492\n",
      "\n",
      "Trial 8/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 96, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.458, P=0.448, R=0.466\n",
      "\n",
      "Trial 9/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 96, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.446, P=0.450, R=0.440\n",
      "\n",
      "Trial 10/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 96, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.425, P=0.404, R=0.444\n",
      "\n",
      "Trial 11/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 96, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.415, P=0.410, R=0.416\n",
      "\n",
      "Trial 12/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.1, 'hidden_dim': 96, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.418, P=0.402, R=0.432\n",
      "\n",
      "Trial 13/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.462, P=0.448, R=0.474\n",
      "\n",
      "Trial 14/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.454, P=0.482, R=0.426\n",
      "\n",
      "Trial 15/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.445, P=0.463, R=0.426\n",
      "\n",
      "Trial 16/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.391, P=0.469, R=0.332\n",
      "\n",
      "Trial 17/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.443, P=0.477, R=0.410\n",
      "\n",
      "Trial 18/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.417, P=0.351, R=0.508\n",
      "\n",
      "Trial 19/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 96, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.450, P=0.366, R=0.582\n",
      "\n",
      "Trial 20/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 96, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.464, P=0.452, R=0.474\n",
      "\n",
      "Trial 21/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 96, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.445, P=0.417, R=0.474\n",
      "\n",
      "Trial 22/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 96, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.409, P=0.379, R=0.440\n",
      "\n",
      "Trial 23/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 96, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.448, P=0.452, R=0.442\n",
      "\n",
      "Trial 24/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 96, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.448, P=0.413, R=0.486\n",
      "\n",
      "Trial 25/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.414, P=0.515, R=0.344\n",
      "\n",
      "Trial 26/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.391, P=0.455, R=0.340\n",
      "\n",
      "Trial 27/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.327, P=0.319, R=0.332\n",
      "\n",
      "Trial 28/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0}\n",
      "Result: F1=0.433, P=0.384, R=0.494\n",
      "\n",
      "Trial 29/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 1e-05}\n",
      "Result: F1=0.417, P=0.366, R=0.480\n",
      "\n",
      "Trial 30/30\n",
      "Testing params: {'batch_size': 64, 'dropout': 0.3, 'hidden_dim': 64, 'learning_rate': 0.005, 'num_layers': 2, 'weight_decay': 0.0001}\n",
      "Result: F1=0.410, P=0.353, R=0.484\n",
      "\n",
      "üèÜ Best result: F1=0.464\n",
      "Best params: {'batch_size': 64, 'dropout': 0.2, 'hidden_dim': 96, 'learning_rate': 0.001, 'num_layers': 2, 'weight_decay': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "class F1HyperparameterTuner:\n",
    "    def __init__(self, param_grid, X_train, y_train, X_val, y_val, device='cuda'):\n",
    "        self.param_grid = list(ParameterGrid(param_grid))\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.device = device\n",
    "        self.results = []\n",
    "        \n",
    "    def objective_function(self, params):\n",
    "        \"\"\"\n",
    "        Train model with given parameters and return validation F1 score\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create model\n",
    "            model = GRUClassifier(\n",
    "                input_dim=self.X_train.shape[2],\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                num_layers=params['num_layers'],\n",
    "                output_dim=1,\n",
    "                dropout=params['dropout']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Calculate class weights\n",
    "            pos_count = np.sum(self.y_train)\n",
    "            neg_count = len(self.y_train) - pos_count\n",
    "            pos_weight = torch.tensor([neg_count / pos_count * params.get('pos_weight_multiplier', 1.0)]).to(self.device)\n",
    "            \n",
    "            # Setup training\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=params['learning_rate'],\n",
    "                weight_decay=params.get('weight_decay', 0)\n",
    "            )\n",
    "            \n",
    "            # Create data loader\n",
    "            train_dataset = TensorDataset(\n",
    "                torch.tensor(self.X_train, dtype=torch.float32),\n",
    "                torch.tensor(self.y_train, dtype=torch.float32)\n",
    "            )\n",
    "            train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "            \n",
    "            # Training loop (reduced epochs for speed)\n",
    "            model.train()\n",
    "            num_epochs = 10  # Reduced for hyperparameter search\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping if specified\n",
    "                    if params.get('gradient_clip'):\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), params['gradient_clip'])\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_val_tensor = torch.tensor(self.X_val, dtype=torch.float32).to(self.device)\n",
    "                logits = model(X_val_tensor)\n",
    "                probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Find best F1 score\n",
    "            from sklearn.metrics import precision_recall_curve, f1_score\n",
    "            precision_vals, recall_vals, thresholds = precision_recall_curve(self.y_val, probs)\n",
    "            f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-8)\n",
    "            best_f1 = np.max(f1_scores)\n",
    "            best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "            \n",
    "            # Calculate final metrics\n",
    "            pred_binary = (probs > best_threshold).astype(int)\n",
    "            precision = precision_score(self.y_val, pred_binary, zero_division=0)\n",
    "            recall = recall_score(self.y_val, pred_binary, zero_division=0)\n",
    "            \n",
    "            result = {\n",
    "                'params': params,\n",
    "                'f1_score': best_f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'threshold': best_threshold,\n",
    "                'val_loss': total_loss / len(train_loader)\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with params {params}: {e}\")\n",
    "            return {\n",
    "                'params': params,\n",
    "                'f1_score': 0,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'threshold': 0.5,\n",
    "                'val_loss': float('inf'),\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def search(self, max_trials=None):\n",
    "        \"\"\"\n",
    "        Run hyperparameter search\n",
    "        \"\"\"\n",
    "        param_combinations = self.param_grid[:max_trials] if max_trials else self.param_grid\n",
    "        \n",
    "        print(f\"Starting hyperparameter search with {len(param_combinations)} combinations...\")\n",
    "        \n",
    "        for i, params in enumerate(param_combinations):\n",
    "            print(f\"\\nTrial {i+1}/{len(param_combinations)}\")\n",
    "            print(f\"Testing params: {params}\")\n",
    "            \n",
    "            result = self.objective_function(params)\n",
    "            self.results.append(result)\n",
    "            \n",
    "            print(f\"Result: F1={result['f1_score']:.3f}, P={result['precision']:.3f}, R={result['recall']:.3f}\")\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Find best result\n",
    "        best_result = max(self.results, key=lambda x: x['f1_score'])\n",
    "        print(f\"\\nüèÜ Best result: F1={best_result['f1_score']:.3f}\")\n",
    "        print(f\"Best params: {best_result['params']}\")\n",
    "        \n",
    "        return self.results, best_result\n",
    "\n",
    "# Usage\n",
    "tuner = F1HyperparameterTuner(\n",
    "    MEDIUM_GRID,  # Start with quick grid\n",
    "    dataset_22['X'], y_2022, dataset_23['X'], y_2023,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "results, best_params = tuner.search(max_trials=30)  # Test 20 combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82f3fb9-2f60-4717-86bf-67ad7f698dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d468b-9689-4d4a-957c-c87cb2113630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
