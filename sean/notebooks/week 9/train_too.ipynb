{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Safety Car Prediction - Step-by-Step Jupyter Notebook Implementation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "from aeon.classification.feature_based import Catch22Classifier\n",
    "from aeon.classification.dummy import DummyClassifier\n",
    "\n",
    "import fastf1\n",
    "\n",
    "from f1_etl import SessionConfig, DataConfig, create_safety_car_dataset, DriverLabelEncoder, FixedVocabTrackStatusEncoder\n",
    "from f1_etl.config import create_multi_session_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0aa022f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetMetadata:\n",
    "    \"\"\"Captures dataset configuration and characteristics\"\"\"\n",
    "    scope: str  # e.g., \"2024_season_races\", \"single_session\", etc.\n",
    "    sessions_config: Dict[str, Any]  # Original sessions configuration\n",
    "    drivers: List[str]\n",
    "    include_weather: bool\n",
    "    window_size: int\n",
    "    prediction_horizon: int\n",
    "    handle_non_numeric: str\n",
    "    handle_missing: bool\n",
    "    missing_strategy: str\n",
    "    normalize: bool\n",
    "    normalization_method: str\n",
    "    target_column: str\n",
    "    \n",
    "    # Dataset characteristics\n",
    "    total_samples: int\n",
    "    n_features: int\n",
    "    n_timesteps: int\n",
    "    feature_names: Optional[List[str]] = None\n",
    "    class_distribution: Optional[Dict[str, int]] = None\n",
    "    \n",
    "    # Processing details\n",
    "    features_used: str = \"all\"  # \"all\", \"speed_only\", \"custom_subset\", etc.\n",
    "    is_multivariate: bool = True\n",
    "    preprocessing_steps: List[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata:\n",
    "    \"\"\"Captures model configuration and hyperparameters\"\"\"\n",
    "    model_type: str  # e.g., \"logistic_regression\", \"random_forest\"\n",
    "    base_estimator: str  # e.g., \"LogisticRegression\", \"RandomForestClassifier\"\n",
    "    wrapper: str = \"Catch22Classifier\"  # Aeon wrapper used\n",
    "    \n",
    "    # Hyperparameters\n",
    "    hyperparameters: Dict[str, Any] = None\n",
    "    class_weights: Optional[Dict[int, float]] = None\n",
    "    custom_weights_applied: bool = False\n",
    "    \n",
    "    # Training details\n",
    "    random_state: Optional[int] = 42\n",
    "    cv_strategy: Optional[str] = None  # If cross-validation used\n",
    "    \n",
    "@dataclass\n",
    "class EvaluationMetadata:\n",
    "    \"\"\"Captures evaluation context and settings\"\"\"\n",
    "    evaluation_id: str\n",
    "    timestamp: str\n",
    "    test_size: float\n",
    "    stratified_split: bool = True\n",
    "    target_class_focus: str = \"safety_car\"\n",
    "    evaluation_metrics: List[str] = None\n",
    "\n",
    "class ModelEvaluationSuite:\n",
    "    \"\"\"Comprehensive model evaluation with metadata tracking and file output\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"evaluation_results\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def evaluate_model(self, \n",
    "                      model, \n",
    "                      model_name: str,\n",
    "                      X_train, X_test, y_train, y_test,\n",
    "                      dataset_metadata: DatasetMetadata,\n",
    "                      model_metadata: ModelMetadata,\n",
    "                      class_names: List[str],\n",
    "                      target_class: str = \"safety_car\",\n",
    "                      save_results: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation with metadata capture\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate evaluation metadata\n",
    "        eval_metadata = EvaluationMetadata(\n",
    "            evaluation_id=f\"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            test_size=len(X_test) / (len(X_train) + len(X_test)),\n",
    "            target_class_focus=target_class,\n",
    "            evaluation_metrics=[\"accuracy\", \"f1_macro\", \"f1_weighted\", \"precision\", \"recall\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVALUATING: {model_name.upper()}\")\n",
    "        print(f\"Evaluation ID: {eval_metadata.evaluation_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            print(\"Training model...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Generate predictions\n",
    "            print(\"Generating predictions...\")\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = None\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                try:\n",
    "                    y_pred_proba = model.predict_proba(X_test)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            metrics = self._calculate_comprehensive_metrics(\n",
    "                y_test, y_pred, y_pred_proba, class_names, target_class\n",
    "            )\n",
    "            \n",
    "            # Create results structure\n",
    "            results = {\n",
    "                \"evaluation_metadata\": asdict(eval_metadata),\n",
    "                \"dataset_metadata\": asdict(dataset_metadata),\n",
    "                \"model_metadata\": asdict(model_metadata),\n",
    "                \"metrics\": metrics,\n",
    "                \"predictions\": {\n",
    "                    \"y_true\": y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),\n",
    "                    \"y_pred\": y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),\n",
    "                    \"y_pred_proba\": y_pred_proba.tolist() if y_pred_proba is not None else None\n",
    "                },\n",
    "                \"class_info\": {\n",
    "                    \"class_names\": class_names,\n",
    "                    \"target_class\": target_class,\n",
    "                    \"target_class_index\": class_names.index(target_class) if target_class in class_names else None\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Print detailed analysis\n",
    "            self._print_detailed_analysis(results)\n",
    "            \n",
    "            # Save results if requested\n",
    "            if save_results:\n",
    "                self._save_results(results, eval_metadata.evaluation_id)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_results = {\n",
    "                \"evaluation_metadata\": asdict(eval_metadata),\n",
    "                \"dataset_metadata\": asdict(dataset_metadata),\n",
    "                \"model_metadata\": asdict(model_metadata),\n",
    "                \"error\": str(e),\n",
    "                \"model_name\": model_name\n",
    "            }\n",
    "            \n",
    "            if save_results:\n",
    "                self._save_results(error_results, eval_metadata.evaluation_id)\n",
    "            \n",
    "            print(f\"ERROR: {str(e)}\")\n",
    "            return error_results\n",
    "    \n",
    "    def _calculate_comprehensive_metrics(self, y_true, y_pred, y_pred_proba, class_names, target_class):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        \n",
    "        # Convert class_names to list consistently at the start\n",
    "        class_names_list = class_names.tolist() if hasattr(class_names, 'tolist') else list(class_names)\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=None, zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Target class specific metrics\n",
    "        target_metrics = {}\n",
    "        target_idx = None\n",
    "        if target_class in class_names_list:\n",
    "            target_idx = class_names_list.index(target_class)\n",
    "            unique_classes = sorted(np.unique(np.concatenate([y_true, y_pred])))\n",
    "            \n",
    "            if target_idx in unique_classes:\n",
    "                target_in_cm = unique_classes.index(target_idx)\n",
    "                if target_in_cm < cm.shape[0] and target_in_cm < cm.shape[1]:\n",
    "                    tp = cm[target_in_cm, target_in_cm]\n",
    "                    fn = cm[target_in_cm, :].sum() - tp\n",
    "                    fp = cm[:, target_in_cm].sum() - tp\n",
    "                    tn = cm.sum() - tp - fn - fp\n",
    "                    \n",
    "                    target_metrics = {\n",
    "                        \"true_positives\": int(tp),\n",
    "                        \"false_negatives\": int(fn),\n",
    "                        \"false_positives\": int(fp),\n",
    "                        \"true_negatives\": int(tn),\n",
    "                        \"precision\": float(precision[target_in_cm] if target_in_cm < len(precision) else 0),\n",
    "                        \"recall\": float(recall[target_in_cm] if target_in_cm < len(recall) else 0),\n",
    "                        \"f1\": float(f1[target_in_cm] if target_in_cm < len(f1) else 0),\n",
    "                        \"support\": int(support[target_in_cm] if target_in_cm < len(support) else 0)\n",
    "                    }\n",
    "        \n",
    "        # Per-class metrics dictionary\n",
    "        per_class_metrics = {}\n",
    "        unique_classes = sorted(np.unique(np.concatenate([y_true, y_pred])))\n",
    "        # Convert class_names to list if it's a numpy array\n",
    "        class_names_list = class_names.tolist() if hasattr(class_names, 'tolist') else list(class_names)\n",
    "        for i, class_idx in enumerate(unique_classes):\n",
    "            if class_idx < len(class_names_list) and i < len(precision):\n",
    "                per_class_metrics[class_names_list[class_idx]] = {\n",
    "                    \"precision\": float(precision[i]),\n",
    "                    \"recall\": float(recall[i]),\n",
    "                    \"f1\": float(f1[i]),\n",
    "                    \"support\": int(support[i])\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            \"overall\": {\n",
    "                \"accuracy\": float(accuracy),\n",
    "                \"f1_macro\": float(f1_macro),\n",
    "                \"f1_weighted\": float(f1_weighted)\n",
    "            },\n",
    "            \"per_class\": per_class_metrics,\n",
    "            \"target_class_metrics\": target_metrics,\n",
    "            \"confusion_matrix\": cm.tolist(),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_true, y_pred, \n",
    "                target_names=[class_names_list[i] for i in unique_classes if i < len(class_names_list)],\n",
    "                zero_division=0, \n",
    "                output_dict=True\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def _print_detailed_analysis(self, results):\n",
    "        \"\"\"Print comprehensive analysis to console\"\"\"\n",
    "        \n",
    "        metrics = results[\"metrics\"]\n",
    "        target_class = results[\"class_info\"][\"target_class\"]\n",
    "        \n",
    "        print(f\"\\n📊 OVERALL PERFORMANCE\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Accuracy:    {metrics['overall']['accuracy']:.4f}\")\n",
    "        print(f\"F1-Macro:    {metrics['overall']['f1_macro']:.4f}\")\n",
    "        print(f\"F1-Weighted: {metrics['overall']['f1_weighted']:.4f}\")\n",
    "        \n",
    "        if metrics['target_class_metrics']:\n",
    "            print(f\"\\n🎯 TARGET CLASS ANALYSIS: {target_class.upper()}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            tm = metrics['target_class_metrics']\n",
    "            print(f\"Precision:       {tm['precision']:.4f}\")\n",
    "            print(f\"Recall:          {tm['recall']:.4f}\")\n",
    "            print(f\"F1-Score:        {tm['f1']:.4f}\")\n",
    "            print(f\"True Positives:  {tm['true_positives']:4d}\")\n",
    "            print(f\"False Negatives: {tm['false_negatives']:4d} (missed {target_class} events)\")\n",
    "            print(f\"False Positives: {tm['false_positives']:4d} (false {target_class} alarms)\")\n",
    "            print(f\"True Negatives:  {tm['true_negatives']:4d}\")\n",
    "        \n",
    "        print(f\"\\n📈 PER-CLASS PERFORMANCE\")\n",
    "        print(f\"{'='*50}\")\n",
    "        for class_name, class_metrics in metrics['per_class'].items():\n",
    "            print(f\"{class_name:12s}: P={class_metrics['precision']:.3f}, \"\n",
    "                  f\"R={class_metrics['recall']:.3f}, \"\n",
    "                  f\"F1={class_metrics['f1']:.3f}, \"\n",
    "                  f\"N={class_metrics['support']}\")\n",
    "        \n",
    "        print(f\"\\n🔍 CONFUSION MATRIX\")\n",
    "        print(f\"{'='*50}\")\n",
    "        cm = np.array(metrics['confusion_matrix'])\n",
    "        class_names_list = results['class_info']['class_names']\n",
    "        # Convert to list if it's a numpy array\n",
    "        if hasattr(class_names_list, 'tolist'):\n",
    "            class_names_list = class_names_list.tolist()\n",
    "        unique_classes = sorted(np.unique(np.concatenate([\n",
    "            results['predictions']['y_true'], \n",
    "            results['predictions']['y_pred']\n",
    "        ])))\n",
    "        present_class_names = [class_names_list[i] for i in unique_classes if i < len(class_names_list)]\n",
    "        \n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[f\"True_{name}\" for name in present_class_names],\n",
    "            columns=[f\"Pred_{name}\" for name in present_class_names]\n",
    "        )\n",
    "        print(cm_df.to_string())\n",
    "    \n",
    "    def _save_results(self, results, evaluation_id):\n",
    "        \"\"\"Save results to JSON and summary text files\"\"\"\n",
    "        \n",
    "        # Save complete results as JSON\n",
    "        json_path = self.output_dir / f\"{evaluation_id}_complete.json\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        # Save human-readable summary\n",
    "        summary_path = self.output_dir / f\"{evaluation_id}_summary.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            self._write_summary_report(results, f)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved:\")\n",
    "        print(f\"  Complete: {json_path}\")\n",
    "        print(f\"  Summary:  {summary_path}\")\n",
    "    \n",
    "    def _write_summary_report(self, results, file_handle):\n",
    "        \"\"\"Write human-readable summary report\"\"\"\n",
    "        \n",
    "        f = file_handle\n",
    "        eval_meta = results[\"evaluation_metadata\"]\n",
    "        dataset_meta = results[\"dataset_metadata\"]\n",
    "        model_meta = results[\"model_metadata\"]\n",
    "        \n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"MODEL EVALUATION REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Evaluation Overview\n",
    "        f.write(\"EVALUATION OVERVIEW\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Evaluation ID: {eval_meta['evaluation_id']}\\n\")\n",
    "        f.write(f\"Timestamp: {eval_meta['timestamp']}\\n\")\n",
    "        f.write(f\"Target Class: {eval_meta['target_class_focus']}\\n\")\n",
    "        f.write(f\"Test Size: {eval_meta['test_size']:.1%}\\n\\n\")\n",
    "        \n",
    "        # Dataset Information\n",
    "        f.write(\"DATASET CONFIGURATION\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Scope: {dataset_meta['scope']}\\n\")\n",
    "        f.write(f\"Drivers: {', '.join(dataset_meta['drivers'])}\\n\")\n",
    "        f.write(f\"Window Size: {dataset_meta['window_size']}\\n\")\n",
    "        f.write(f\"Prediction Horizon: {dataset_meta['prediction_horizon']}\\n\")\n",
    "        f.write(f\"Features Used: {dataset_meta['features_used']}\\n\")\n",
    "        f.write(f\"Multivariate: {dataset_meta['is_multivariate']}\\n\")\n",
    "        f.write(f\"Total Samples: {dataset_meta['total_samples']:,}\\n\")\n",
    "        f.write(f\"Shape: ({dataset_meta['total_samples']}, {dataset_meta['n_features']}, {dataset_meta['n_timesteps']})\\n\")\n",
    "        if dataset_meta['class_distribution']:\n",
    "            f.write(\"Class Distribution:\\n\")\n",
    "            for class_name, count in dataset_meta['class_distribution'].items():\n",
    "                f.write(f\"  {class_name}: {count:,}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Model Configuration\n",
    "        f.write(\"MODEL CONFIGURATION\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Model Type: {model_meta['model_type']}\\n\")\n",
    "        f.write(f\"Base Estimator: {model_meta['base_estimator']}\\n\")\n",
    "        f.write(f\"Wrapper: {model_meta['wrapper']}\\n\")\n",
    "        f.write(f\"Custom Weights: {model_meta['custom_weights_applied']}\\n\")\n",
    "        if model_meta['hyperparameters']:\n",
    "            f.write(\"Hyperparameters:\\n\")\n",
    "            for param, value in model_meta['hyperparameters'].items():\n",
    "                f.write(f\"  {param}: {value}\\n\")\n",
    "        if model_meta['class_weights']:\n",
    "            f.write(\"Class Weights:\\n\")\n",
    "            for class_idx, weight in model_meta['class_weights'].items():\n",
    "                f.write(f\"  Class {class_idx}: {weight}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Performance Results\n",
    "        if \"metrics\" in results:\n",
    "            metrics = results[\"metrics\"]\n",
    "            target_class = results[\"class_info\"][\"target_class\"]\n",
    "            \n",
    "            f.write(\"PERFORMANCE RESULTS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Overall Accuracy: {metrics['overall']['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"F1-Macro: {metrics['overall']['f1_macro']:.4f}\\n\")\n",
    "            f.write(f\"F1-Weighted: {metrics['overall']['f1_weighted']:.4f}\\n\\n\")\n",
    "            \n",
    "            if metrics['target_class_metrics']:\n",
    "                f.write(f\"TARGET CLASS ANALYSIS: {target_class.upper()}\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                tm = metrics['target_class_metrics']\n",
    "                f.write(f\"Precision: {tm['precision']:.4f}\\n\")\n",
    "                f.write(f\"Recall: {tm['recall']:.4f}\\n\")\n",
    "                f.write(f\"F1-Score: {tm['f1']:.4f}\\n\")\n",
    "                f.write(f\"True Positives: {tm['true_positives']:,}\\n\")\n",
    "                f.write(f\"False Negatives: {tm['false_negatives']:,} (missed events)\\n\")\n",
    "                f.write(f\"False Positives: {tm['false_positives']:,} (false alarms)\\n\")\n",
    "                f.write(f\"True Negatives: {tm['true_negatives']:,}\\n\\n\")\n",
    "            \n",
    "            f.write(\"PER-CLASS PERFORMANCE\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Support':<10}\\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\")\n",
    "            for class_name, class_metrics in metrics['per_class'].items():\n",
    "                f.write(f\"{class_name:<12} {class_metrics['precision']:<10.3f} \"\n",
    "                       f\"{class_metrics['recall']:<10.3f} {class_metrics['f1']:<10.3f} \"\n",
    "                       f\"{class_metrics['support']:<10}\\n\")\n",
    "        \n",
    "        else:\n",
    "            f.write(\"ERROR OCCURRED\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Error: {results.get('error', 'Unknown error')}\\n\")\n",
    "\n",
    "# Helper functions for easy metadata creation\n",
    "def create_dataset_metadata_from_f1_config(dataset_config, dataset, processing_config=None, features_used=\"all\"):\n",
    "    \"\"\"\n",
    "    Create DatasetMetadata from F1 ETL configuration and dataset object\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_config : DataConfig\n",
    "        The F1 ETL DataConfig object\n",
    "    dataset : dict\n",
    "        The dataset dictionary returned by create_safety_car_dataset\n",
    "    processing_config : dict, optional\n",
    "        The processing config from dataset['config'] if available\n",
    "    features_used : str\n",
    "        Description of which features were used\n",
    "    \"\"\"\n",
    "    \n",
    "    X = dataset['X']\n",
    "    y = dataset['y']\n",
    "    \n",
    "    # Use processing config from dataset if available\n",
    "    if processing_config is None and 'config' in dataset:\n",
    "        processing_config = dataset['config']\n",
    "    \n",
    "    # Determine scope description\n",
    "    sessions = dataset_config.sessions if hasattr(dataset_config, 'sessions') else []\n",
    "    if len(sessions) == 1:\n",
    "        session = sessions[0]\n",
    "        year = getattr(session, 'year', 'unknown')\n",
    "        race = getattr(session, 'race', 'unknown')\n",
    "        session_type = getattr(session, 'session_type', 'unknown')\n",
    "        scope = f\"single_session_{year}_{race}_{session_type}\".replace(' ', '_')\n",
    "    elif len(sessions) > 1:\n",
    "        years = list(set(getattr(s, 'year', None) for s in sessions))\n",
    "        years = [y for y in years if y is not None]\n",
    "        session_types = list(set(getattr(s, 'session_type', None) for s in sessions))\n",
    "        session_types = [st for st in session_types if st is not None]\n",
    "        \n",
    "        if years and session_types:\n",
    "            year_str = '-'.join(map(str, sorted(years)))\n",
    "            type_str = '_'.join(sorted(session_types))\n",
    "            scope = f\"multi_session_{year_str}_{type_str}_{len(sessions)}sessions\"\n",
    "        else:\n",
    "            scope = f\"multi_session_{len(sessions)}sessions\"\n",
    "    else:\n",
    "        scope = \"unknown_scope\"\n",
    "    \n",
    "    # Get class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    label_encoder = dataset.get('label_encoder')\n",
    "    class_dist = {}\n",
    "    if label_encoder and hasattr(label_encoder, 'class_to_idx'):\n",
    "        idx_to_class = {v: k for k, v in label_encoder.class_to_idx.items()}\n",
    "        class_dist = {str(idx_to_class.get(idx, f\"class_{idx}\")): int(count) \n",
    "                     for idx, count in zip(unique, counts)}\n",
    "    elif label_encoder and hasattr(label_encoder, 'classes_'):\n",
    "        # Standard sklearn LabelEncoder\n",
    "        class_names = label_encoder.classes_\n",
    "        class_dist = {str(class_names[idx]): int(count) \n",
    "                     for idx, count in zip(unique, counts) if idx < len(class_names)}\n",
    "    \n",
    "    # Extract feature names if available\n",
    "    feature_names = None\n",
    "    if processing_config and 'feature_names' in processing_config:\n",
    "        feature_names = processing_config['feature_names']\n",
    "    elif 'metadata' in dataset and dataset['metadata']:\n",
    "        # Try to get from first metadata entry\n",
    "        meta_entry = dataset['metadata'][0] if isinstance(dataset['metadata'], list) else dataset['metadata']\n",
    "        if isinstance(meta_entry, dict) and 'features_used' in meta_entry:\n",
    "            feature_names = meta_entry['features_used']\n",
    "    \n",
    "    # Get preprocessing steps\n",
    "    preprocessing_steps = []\n",
    "    if processing_config:\n",
    "        if processing_config.get('missing_values_handled', False):\n",
    "            preprocessing_steps.append(f\"missing_values_handled_{processing_config.get('missing_strategy', 'unknown')}\")\n",
    "        if processing_config.get('normalization_applied', False):\n",
    "            preprocessing_steps.append(f\"normalized_{processing_config.get('normalization_method', 'unknown')}\")\n",
    "    \n",
    "    return DatasetMetadata(\n",
    "        scope=scope,\n",
    "        sessions_config=[{\n",
    "            'year': getattr(s, 'year', None),\n",
    "            'race': getattr(s, 'race', None), \n",
    "            'session_type': getattr(s, 'session_type', None)\n",
    "        } for s in sessions],\n",
    "        drivers=getattr(dataset_config, 'drivers', []),\n",
    "        include_weather=getattr(dataset_config, 'include_weather', False),\n",
    "        window_size=processing_config.get('window_size', 100) if processing_config else 100,\n",
    "        prediction_horizon=processing_config.get('prediction_horizon', 10) if processing_config else 10,\n",
    "        handle_non_numeric=processing_config.get('handle_non_numeric', 'encode') if processing_config else 'encode',\n",
    "        handle_missing=processing_config.get('handle_missing', True) if processing_config else True,\n",
    "        missing_strategy=processing_config.get('missing_strategy', 'forward_fill') if processing_config else 'forward_fill',\n",
    "        normalize=processing_config.get('normalize', True) if processing_config else True,\n",
    "        normalization_method=processing_config.get('normalization_method', 'per_sequence') if processing_config else 'per_sequence',\n",
    "        target_column=processing_config.get('target_column', 'TrackStatus') if processing_config else 'TrackStatus',\n",
    "        total_samples=X.shape[0],\n",
    "        n_features=X.shape[1] if len(X.shape) > 1 else 1,\n",
    "        n_timesteps=X.shape[2] if len(X.shape) > 2 else X.shape[1],\n",
    "        feature_names=feature_names,\n",
    "        class_distribution=class_dist,\n",
    "        features_used=features_used,\n",
    "        is_multivariate=len(X.shape) > 2 and X.shape[1] > 1,\n",
    "        preprocessing_steps=preprocessing_steps\n",
    "    )\n",
    "\n",
    "def create_metadata_from_f1_dataset(data_config, dataset, features_used=\"multivariate_all_9_features\"):\n",
    "    \"\"\"\n",
    "    Convenience function to create metadata from F1 dataset\n",
    "    \"\"\"\n",
    "    return create_dataset_metadata_from_f1_config(\n",
    "        dataset_config=data_config,\n",
    "        dataset=dataset,\n",
    "        processing_config=dataset.get('config'),  # Use the config from the dataset\n",
    "        features_used=features_used\n",
    "    )\n",
    "\n",
    "def create_model_metadata(model_name, model, class_weights=None):\n",
    "    \"\"\"Create ModelMetadata from model configuration\"\"\"\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    hyperparams = {}\n",
    "    if hasattr(model, 'estimator') and hasattr(model.estimator, 'get_params'):\n",
    "        hyperparams = model.estimator.get_params()\n",
    "    elif hasattr(model, 'get_params'):\n",
    "        hyperparams = model.get_params()\n",
    "    \n",
    "    # Determine base estimator name\n",
    "    base_estimator = \"Unknown\"\n",
    "    if hasattr(model, 'estimator'):\n",
    "        base_estimator = model.estimator.__class__.__name__\n",
    "    else:\n",
    "        base_estimator = model.__class__.__name__\n",
    "    \n",
    "    return ModelMetadata(\n",
    "        model_type=model_name,\n",
    "        base_estimator=base_estimator,\n",
    "        wrapper=\"Catch22Classifier\" if hasattr(model, 'estimator') else \"Direct\",\n",
    "        hyperparameters=hyperparams,\n",
    "        class_weights=class_weights,\n",
    "        custom_weights_applied=class_weights is not None,\n",
    "        random_state=hyperparams.get('random_state', None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b13c1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the evaluation suite\n",
    "evaluator = ModelEvaluationSuite(output_dir=\"evaluation_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e851996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_2024_season = create_multi_session_configs(\n",
    "#     year=2024, \n",
    "#     session_types=['R'], \n",
    "#     include_testing=False\n",
    "# )\n",
    "# data_config = DataConfig(\n",
    "#     sessions=sessions_2024_season, \n",
    "#     drivers=['2'], \n",
    "#     include_weather=False\n",
    "# )\n",
    "data_config = DataConfig(\n",
    "    sessions=[SessionConfig(2024, 'Saudia Arabian Grand Prix', 'R')],\n",
    "    drivers=['1'],\n",
    "    include_weather=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1878eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 21:38:26,868 - f1_etl - INFO - Preprocessing configuration:\n",
      "2025-07-02 21:38:26,869 - f1_etl - INFO -   Missing values: enabled (forward_fill)\n",
      "2025-07-02 21:38:26,870 - f1_etl - INFO -   Normalization: enabled (per_sequence)\n",
      "events      WARNING \tCorrecting user input 'Saudia Arabian Grand Prix' to 'Saudi Arabian Grand Prix'\n",
      "core           INFO \tLoading data for Saudi Arabian Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 Saudia Arabian Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '16', '81', '14', '63', '38', '4', '44', '27', '23', '20', '31', '2', '22', '3', '77', '24', '18', '10']\n",
      "2025-07-02 21:38:29,525 - f1_etl - INFO - Creating new fixed vocabulary encoder\n",
      "2025-07-02 21:38:29,559 - f1_etl - INFO - Processing 66149 total telemetry rows\n",
      "2025-07-02 21:38:29,560 - f1_etl - INFO - Grouping by: ['SessionId', 'Driver']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Track Status Analysis (training_data):\n",
      "   green       : 59396 samples ( 89.8%)\n",
      "   safety_car  :  3135 samples (  4.7%)\n",
      "   yellow      :  3618 samples (  5.5%)\n",
      "   Missing classes: [np.str_('red'), np.str_('unknown'), np.str_('vsc'), np.str_('vsc_ending')]\n",
      "✅ FixedVocabTrackStatusEncoder fitted\n",
      "   Classes seen: ['green', 'safety_car', 'yellow']\n",
      "   Total classes: 7\n",
      "   Output mode: integer labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 21:38:29,950 - f1_etl - INFO - Total sequences generated: 1321\n",
      "2025-07-02 21:38:29,959 - f1_etl - INFO - Generated 1321 sequences with shape (1321, 100, 9)\n",
      "2025-07-02 21:38:29,963 - f1_etl - INFO - No missing values detected, skipping imputation\n",
      "2025-07-02 21:38:29,964 - f1_etl - INFO - Applying normalization with method: per_sequence\n",
      "2025-07-02 21:38:29,994 - f1_etl - INFO - Final dataset summary:\n",
      "2025-07-02 21:38:29,994 - f1_etl - INFO -   Sequences: 1321\n",
      "2025-07-02 21:38:29,995 - f1_etl - INFO -   Features: 9\n",
      "2025-07-02 21:38:29,995 - f1_etl - INFO -   Classes: 7 (integer)\n",
      "2025-07-02 21:38:29,996 - f1_etl - INFO -   Label shape: (1321,)\n",
      "2025-07-02 21:38:29,996 - f1_etl - INFO -     green       :  1188 samples ( 89.9%)\n",
      "2025-07-02 21:38:29,996 - f1_etl - INFO -     safety_car  :    62 samples (  4.7%)\n",
      "2025-07-02 21:38:29,996 - f1_etl - INFO -     yellow      :    71 samples (  5.4%)\n"
     ]
    }
   ],
   "source": [
    "dataset = create_safety_car_dataset(\n",
    "    config=data_config,\n",
    "    window_size=100,\n",
    "    prediction_horizon=10,\n",
    "    handle_non_numeric=\"encode\",\n",
    "    handle_missing=True,\n",
    "    missing_strategy=\"forward_fill\",\n",
    "    normalize=True,\n",
    "    normalization_method=\"per_sequence\",\n",
    "    target_column=\"TrackStatus\",\n",
    "    enable_debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f2275ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset metadata from configuration\n",
    "dataset_metadata = create_metadata_from_f1_dataset(\n",
    "    data_config=data_config,\n",
    "    dataset=dataset,\n",
    "    features_used=\"multivariate_all_9_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8ec7473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create class names list from your label encoder\n",
    "class_names = list(dataset['label_encoder'].class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a07773d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CLASS DISTRIBUTION ===\n",
      "green       :   950 samples ( 90.0%)\n",
      "safety_car  :    49 samples (  4.6%)\n",
      "yellow      :    57 samples (  5.4%)\n",
      "\n",
      "Imbalance ratio: 19.4:1\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(dataset, test_size=0.2):\n",
    "    \"\"\"Prepare train/test splits and convert to Aeon format\"\"\"\n",
    "    \n",
    "    X = dataset['X']  # Shape: (n_samples, n_timesteps, n_features)\n",
    "    y = dataset['y']  # Encoded labels\n",
    "    \n",
    "    # Convert to Aeon format: (n_samples, n_features, n_timesteps)\n",
    "    X_aeon = X.transpose(0, 2, 1)\n",
    "    \n",
    "    # Use only Speed feature (index 0) for simplicity\n",
    "    # X_speed = X_aeon[:, 0:1, :]  # Keep 3D: (n_samples, 1, n_timesteps)\n",
    "    \n",
    "    # Train/test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        # X_speed, y, test_size=test_size, random_state=42, stratify=y\n",
    "        X_aeon, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def analyze_class_distribution(dataset, y_train):\n",
    "    \"\"\"Analyze and display class distribution\"\"\"\n",
    "    \n",
    "    # Get class names\n",
    "    label_encoder = dataset['label_encoder']\n",
    "    class_names = label_encoder.get_classes()\n",
    "    \n",
    "    # Count classes\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    \n",
    "    print(\"\\n=== CLASS DISTRIBUTION ===\")\n",
    "    for class_id, count in zip(unique, counts):\n",
    "        class_name = class_names[class_id] if class_id < len(class_names) else f\"Class_{class_id}\"\n",
    "        percentage = count / len(y_train) * 100\n",
    "        print(f\"{class_name:12s}: {count:5d} samples ({percentage:5.1f}%)\")\n",
    "    \n",
    "    imbalance_ratio = max(counts) / min(counts)\n",
    "    print(f\"\\nImbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "    \n",
    "    return class_names, dict(zip(unique, counts))\n",
    "\n",
    "# Example usage:\n",
    "X_train, X_test, y_train, y_test = prepare_data(dataset)\n",
    "class_names, class_dist = analyze_class_distribution(dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5d81b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1056, 9, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f453258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.str_('green'): 0,\n",
       " np.str_('red'): 1,\n",
       " np.str_('safety_car'): 2,\n",
       " np.str_('unknown'): 3,\n",
       " np.str_('vsc'): 4,\n",
       " np.str_('vsc_ending'): 5,\n",
       " np.str_('yellow'): 6}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enc = dataset['label_encoder']\n",
    "y_enc.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "938bbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = {\n",
    "#     y_enc.class_to_idx['green']: 1.0,        # 0\n",
    "#     y_enc.class_to_idx['red']: 10.0,         # 1  \n",
    "#     y_enc.class_to_idx['safety_car']: 20.0,  # 2 (your target class)\n",
    "#     y_enc.class_to_idx['unknown']: 5.0,      # 3\n",
    "#     y_enc.class_to_idx['vsc']: 30.0,         # 4\n",
    "#     y_enc.class_to_idx['vsc_ending']: 100.0, # 5\n",
    "#     y_enc.class_to_idx['yellow']: 8.0        # 6\n",
    "# }\n",
    "\n",
    "class_weight = {\n",
    "    y_enc.class_to_idx['green']: 1.0,\n",
    "    y_enc.class_to_idx['red']: 25.0,         \n",
    "    y_enc.class_to_idx['safety_car']: 100.0,  # Much higher\n",
    "    y_enc.class_to_idx['unknown']: 25.0,      \n",
    "    y_enc.class_to_idx['vsc']: 50.0,\n",
    "    y_enc.class_to_idx['vsc_ending']: 50.0,\n",
    "    y_enc.class_to_idx['yellow']: 25.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f362306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(class_weight: Optional[Dict] = None):\n",
    "    \"\"\"Create dictionary of models to test\"\"\"\n",
    "\n",
    "    cls_weight = 'balanced' if class_weight is None else class_weight\n",
    "    \n",
    "    models = {\n",
    "        \"dummy_frequent\": DummyClassifier(strategy='most_frequent'),\n",
    "        \n",
    "        \"dummy_stratified\": DummyClassifier(strategy='stratified'),\n",
    "        \n",
    "        \"logistic_regression\": Catch22Classifier(\n",
    "            estimator=LogisticRegression(\n",
    "                random_state=42, \n",
    "                max_iter=3000, \n",
    "                # solver='liblinear',\n",
    "                solver='saga',\n",
    "                penalty='l1',\n",
    "                C=0.1,\n",
    "                class_weight=cls_weight,\n",
    "            ),\n",
    "            outlier_norm=True,\n",
    "            random_state=42,\n",
    "        ),\n",
    "        \n",
    "        \"random_forest\": Catch22Classifier(\n",
    "            estimator=RandomForestClassifier(\n",
    "                n_estimators=100, \n",
    "                random_state=42,\n",
    "                class_weight=cls_weight,\n",
    "                max_depth=10\n",
    "            ),\n",
    "            outlier_norm=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "def train_single_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate a single model\"\"\"\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        results = {\n",
    "            \"model_name\": model_name,\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"f1_macro\": f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "            \"f1_weighted\": f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "            \"predictions\": y_pred,\n",
    "            \"true_labels\": y_test\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"  F1-Macro: {results['f1_macro']:.4f}\")\n",
    "        print(f\"  F1-Weighted: {results['f1_weighted']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {str(e)}\")\n",
    "        return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "# Example usage:\n",
    "# models = create_models()\n",
    "# results = {}\n",
    "# for model_name, model in models.items():\n",
    "#     results[model_name] = train_single_model(model, model_name, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b491b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = create_models(class_weight=class_weight)\n",
    "results = {}\n",
    "\n",
    "model_name = 'logistic_regression'\n",
    "model = models[model_name]\n",
    "\n",
    "# results[model_name] = train_single_model(model, model_name, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63740ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model metadata\n",
    "model_metadata = create_model_metadata(\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    "    class_weights=class_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91f69f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING: LOGISTIC_REGRESSION\n",
      "Evaluation ID: logistic_regression_20250702_214809\n",
      "================================================================================\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seansica/Documents/Development/mids/capstone/datasci-210-2025-summer-formula1/sean/notebooks/.venv/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.8113\n",
      "F1-Macro:    0.6413\n",
      "F1-Weighted: 0.8477\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       0.2391\n",
      "Recall:          0.8462\n",
      "F1-Score:        0.3729\n",
      "True Positives:    11\n",
      "False Negatives:    2 (missed safety_car events)\n",
      "False Positives:   35 (false safety_car alarms)\n",
      "True Negatives:   217\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=0.985, R=0.803, F1=0.884, N=238\n",
      "safety_car  : P=0.239, R=0.846, F1=0.373, N=13\n",
      "yellow      : P=0.520, R=0.929, F1=0.667, N=14\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green              191               35           12\n",
      "True_safety_car           2               11            0\n",
      "True_yellow               1                0           13\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/logistic_regression_20250702_214809_complete.json\n",
      "  Summary:  evaluation_results/logistic_regression_20250702_214809_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive evaluation\n",
    "results = evaluator.evaluate_model(\n",
    "    model=model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    dataset_metadata=dataset_metadata,\n",
    "    model_metadata=model_metadata,\n",
    "    class_names=list(class_names),  # Ensure it's a list\n",
    "    target_class=\"safety_car\",\n",
    "    save_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd379d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 22:00:03,094 - f1_etl - INFO - Preprocessing configuration:\n",
      "2025-07-02 22:00:03,095 - f1_etl - INFO -   Missing values: enabled (forward_fill)\n",
      "2025-07-02 22:00:03,095 - f1_etl - INFO -   Normalization: enabled (per_sequence)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2025 Saudi Arabian Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Saudi Arabian Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tNo cached data found for session_info. Loading data...\n",
      "_api           INFO \tFetching session info data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for driver_info. Loading data...\n",
      "_api           INFO \tFetching driver list...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for session_status_data. Loading data...\n",
      "_api           INFO \tFetching session status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for lap_count. Loading data...\n",
      "_api           INFO \tFetching lap count data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for track_status_data. Loading data...\n",
      "_api           INFO \tFetching track status data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for _extended_timing_data. Loading data...\n",
      "_api           INFO \tFetching timing data...\n",
      "_api           INFO \tParsing timing data...\n",
      "_api        WARNING \tFailed to align laps for drivers: ['22']\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for timing_app_data. Loading data...\n",
      "_api           INFO \tFetching timing app data...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tNo cached data found for car_data. Loading data...\n",
      "_api           INFO \tFetching car data...\n",
      "_api           INFO \tParsing car data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for position_data. Loading data...\n",
      "_api           INFO \tFetching position data...\n",
      "_api           INFO \tParsing position data...\n",
      "_api        WARNING \tDriver 241: Position data is incomplete!\n",
      "_api        WARNING \tDriver 242: Position data is incomplete!\n",
      "_api        WARNING \tDriver 243: Position data is incomplete!\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for weather_data. Loading data...\n",
      "_api           INFO \tFetching weather data...\n",
      "req            INFO \tData has been written to cache!\n",
      "req            INFO \tNo cached data found for race_control_messages. Loading data...\n",
      "_api           INFO \tFetching race control messages...\n",
      "req            INFO \tData has been written to cache!\n",
      "core           INFO \tFinished loading data for 20 drivers: ['81', '1', '16', '4', '63', '12', '44', '55', '23', '6', '14', '30', '87', '31', '27', '18', '7', '5', '22', '10']\n",
      "2025-07-02 22:00:16,393 - f1_etl - INFO - Creating new fixed vocabulary encoder\n",
      "2025-07-02 22:00:16,412 - f1_etl - INFO - Processing 63330 total telemetry rows\n",
      "2025-07-02 22:00:16,412 - f1_etl - INFO - Grouping by: ['SessionId', 'Driver']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Track Status Analysis (training_data):\n",
      "   green       : 60283 samples ( 95.2%)\n",
      "   safety_car  :  2777 samples (  4.4%)\n",
      "   yellow      :   270 samples (  0.4%)\n",
      "   Missing classes: [np.str_('red'), np.str_('unknown'), np.str_('vsc'), np.str_('vsc_ending')]\n",
      "✅ FixedVocabTrackStatusEncoder fitted\n",
      "   Classes seen: ['green', 'safety_car', 'yellow']\n",
      "   Total classes: 7\n",
      "   Output mode: integer labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 22:00:16,681 - f1_etl - INFO - Total sequences generated: 1265\n",
      "2025-07-02 22:00:16,695 - f1_etl - INFO - Generated 1265 sequences with shape (1265, 100, 9)\n",
      "2025-07-02 22:00:16,697 - f1_etl - INFO - No missing values detected, skipping imputation\n",
      "2025-07-02 22:00:16,697 - f1_etl - INFO - Applying normalization with method: per_sequence\n",
      "2025-07-02 22:00:16,727 - f1_etl - INFO - Final dataset summary:\n",
      "2025-07-02 22:00:16,728 - f1_etl - INFO -   Sequences: 1265\n",
      "2025-07-02 22:00:16,728 - f1_etl - INFO -   Features: 9\n",
      "2025-07-02 22:00:16,729 - f1_etl - INFO -   Classes: 7 (integer)\n",
      "2025-07-02 22:00:16,729 - f1_etl - INFO -   Label shape: (1265,)\n",
      "2025-07-02 22:00:16,730 - f1_etl - INFO -     green       :  1204 samples ( 95.2%)\n",
      "2025-07-02 22:00:16,730 - f1_etl - INFO -     safety_car  :    55 samples (  4.3%)\n",
      "2025-07-02 22:00:16,730 - f1_etl - INFO -     yellow      :     6 samples (  0.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.7296\n",
      "F1-Macro:    0.3706\n",
      "F1-Weighted: 0.8099\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       0.1673\n",
      "Recall:          0.7455\n",
      "F1-Score:        0.2733\n",
      "True Positives:    41\n",
      "False Negatives:   14 (missed safety_car events)\n",
      "False Positives:  204 (false safety_car alarms)\n",
      "True Negatives:  1006\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=0.980, R=0.733, F1=0.838, N=1204\n",
      "safety_car  : P=0.167, R=0.745, F1=0.273, N=55\n",
      "yellow      : P=0.000, R=0.000, F1=0.000, N=6\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green              882              204          118\n",
      "True_safety_car          12               41            2\n",
      "True_yellow               6                0            0\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/logistic_regression_external_test_20250702_220021_complete.json\n",
      "  Summary:  evaluation_results/logistic_regression_external_test_20250702_220021_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# 5. For evaluating against a different test set\n",
    "def evaluate_on_different_test_set(model, test_config, evaluator, model_metadata, dataset_metadata, class_names):\n",
    "    \"\"\"Evaluate trained model on a different test set\"\"\"\n",
    "    \n",
    "    # Load different test set\n",
    "    test_dataset = create_safety_car_dataset(\n",
    "        config=test_config,\n",
    "        window_size=dataset_metadata.window_size,\n",
    "        prediction_horizon=dataset_metadata.prediction_horizon,\n",
    "        handle_non_numeric=\"encode\",\n",
    "        handle_missing=True,\n",
    "        missing_strategy=\"forward_fill\",\n",
    "        normalize=True,\n",
    "        normalization_method=\"per_sequence\",\n",
    "        target_column=\"TrackStatus\",\n",
    "        enable_debug=False\n",
    "    )\n",
    "    \n",
    "    X_test_new = test_dataset['X'].transpose(0, 2, 1)  # Convert to Aeon format\n",
    "    y_test_new = test_dataset['y']\n",
    "    \n",
    "    # Create new dataset metadata for this test set\n",
    "    test_dataset_metadata = create_dataset_metadata_from_config(\n",
    "        dataset_config=test_config,\n",
    "        dataset=test_dataset,\n",
    "        features_used=dataset_metadata.features_used\n",
    "    )\n",
    "    test_dataset_metadata.scope = f\"external_test_{test_dataset_metadata.scope}\"\n",
    "    \n",
    "    # Evaluate without retraining (model already fitted)\n",
    "    try:\n",
    "        y_pred_new = model.predict(X_test_new)\n",
    "        y_pred_proba = None\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            try:\n",
    "                y_pred_proba = model.predict_proba(X_test_new)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Create evaluation metadata\n",
    "        eval_metadata = EvaluationMetadata(\n",
    "            evaluation_id=f\"{model_metadata.model_type}_external_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            test_size=1.0,  # 100% test data\n",
    "            target_class_focus=\"safety_car\",\n",
    "            evaluation_metrics=[\"accuracy\", \"f1_macro\", \"f1_weighted\", \"precision\", \"recall\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics directly\n",
    "        metrics = evaluator._calculate_comprehensive_metrics(\n",
    "            y_test_new, y_pred_new, y_pred_proba, list(class_names), \"safety_car\"\n",
    "        )\n",
    "        \n",
    "        # Create results structure\n",
    "        results = {\n",
    "            \"evaluation_metadata\": asdict(eval_metadata),\n",
    "            \"dataset_metadata\": asdict(test_dataset_metadata),\n",
    "            \"model_metadata\": asdict(model_metadata),\n",
    "            \"metrics\": metrics,\n",
    "            \"predictions\": {\n",
    "                \"y_true\": y_test_new.tolist(),\n",
    "                \"y_pred\": y_pred_new.tolist(),\n",
    "                \"y_pred_proba\": y_pred_proba.tolist() if y_pred_proba is not None else None\n",
    "            },\n",
    "            \"class_info\": {\n",
    "                \"class_names\": list(class_names),\n",
    "                \"target_class\": \"safety_car\",\n",
    "                \"target_class_index\": list(class_names).index(\"safety_car\")\n",
    "            },\n",
    "            \"note\": \"External test set evaluation - model was pre-trained\"\n",
    "        }\n",
    "        \n",
    "        # Print and save results\n",
    "        evaluator._print_detailed_analysis(results)\n",
    "        evaluator._save_results(results, eval_metadata.evaluation_id)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating on external test set: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# Train on 2024 Saudi Arabian GP, test on 2025 Saudi Arabian GP\n",
    "test_config = DataConfig(\n",
    "    sessions=[SessionConfig(2025, 'Saudi Arabian Grand Prix', 'R')],\n",
    "    drivers=['1'],\n",
    "    include_weather=False\n",
    ")\n",
    "\n",
    "external_results = evaluate_on_different_test_set(\n",
    "    model=model,  # Your already trained model\n",
    "    test_config=test_config,\n",
    "    evaluator=evaluator,\n",
    "    model_metadata=model_metadata,\n",
    "    dataset_metadata=dataset_metadata,\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "076cae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Batch evaluation of multiple models\n",
    "def run_comprehensive_model_comparison(X_train, X_test, y_train, y_test, dataset_metadata, class_names, evaluator, class_weight=None):\n",
    "    \"\"\"Run evaluation on all models and save results\"\"\"\n",
    "    \n",
    "    models = create_models(class_weight=class_weight)\n",
    "    all_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"EVALUATING MODEL: {model_name}\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        # Create model-specific metadata\n",
    "        model_metadata = create_model_metadata(\n",
    "            model_name=model_name,\n",
    "            model=model,\n",
    "            class_weights=class_weight if 'logistic' in model_name or 'forest' in model_name else None\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluator.evaluate_model(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "            dataset_metadata=dataset_metadata,\n",
    "            model_metadata=model_metadata,\n",
    "            class_names=list(class_names),\n",
    "            target_class=\"safety_car\",\n",
    "            save_results=True\n",
    "        )\n",
    "        \n",
    "        all_results[model_name] = results\n",
    "    \n",
    "    # Save comparison summary\n",
    "    comparison_id = f\"model_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    comparison_path = evaluator.output_dir / f\"{comparison_id}_comparison.json\"\n",
    "    \n",
    "    with open(comparison_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"comparison_id\": comparison_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"models_compared\": list(all_results.keys()),\n",
    "            \"results\": all_results\n",
    "        }, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n📊 Model comparison saved: {comparison_path}\")\n",
    "    return all_results\n",
    "\n",
    "# 7. Example with different feature configurations\n",
    "def evaluate_feature_configurations(dataset, dataset_metadata, class_names, evaluator, class_weight=None):\n",
    "    \"\"\"Evaluate same model with different feature configurations\"\"\"\n",
    "    \n",
    "    X = dataset['X']  # Shape: (n_samples, n_timesteps, n_features)\n",
    "    y = dataset['y']\n",
    "    X_aeon = X.transpose(0, 2, 1)  # Convert to Aeon format\n",
    "    \n",
    "    configurations = [\n",
    "        {\n",
    "            \"name\": \"speed_only\",\n",
    "            \"X_data\": X_aeon[:, 0:1, :],  # Speed only\n",
    "            \"description\": \"univariate_speed_only\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"speed_and_throttle\", \n",
    "            \"X_data\": X_aeon[:, [0, 3], :],  # Speed and throttle (assuming throttle is index 3)\n",
    "            \"description\": \"bivariate_speed_throttle\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"all_features\",\n",
    "            \"X_data\": X_aeon,  # All features\n",
    "            \"description\": \"multivariate_all_9_features\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    base_model_name = \"logistic_regression\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for config in configurations:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FEATURE CONFIGURATION: {config['name'].upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Split data for this configuration\n",
    "        X_train_config, X_test_config, y_train_config, y_test_config = train_test_split(\n",
    "            config[\"X_data\"], y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Create a copy of dataset metadata and update it\n",
    "        from copy import deepcopy\n",
    "        config_dataset_metadata = deepcopy(dataset_metadata)\n",
    "        config_dataset_metadata.features_used = config[\"description\"]\n",
    "        config_dataset_metadata.n_features = config[\"X_data\"].shape[1]\n",
    "        config_dataset_metadata.is_multivariate = config[\"X_data\"].shape[1] > 1\n",
    "        \n",
    "        # Create fresh model instance\n",
    "        model_config = Catch22Classifier(\n",
    "            estimator=LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=3000,\n",
    "                solver='saga',\n",
    "                penalty='l1',\n",
    "                C=0.1,\n",
    "                class_weight=class_weight\n",
    "            ),\n",
    "            outlier_norm=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Update model metadata\n",
    "        config_model_metadata = create_model_metadata(\n",
    "            model_name=f\"{base_model_name}_{config['name']}\",\n",
    "            model=model_config,\n",
    "            class_weights=class_weight\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluator.evaluate_model(\n",
    "            model=model_config,\n",
    "            model_name=f\"{base_model_name}_{config['name']}\",\n",
    "            X_train=X_train_config,\n",
    "            X_test=X_test_config,\n",
    "            y_train=y_train_config,\n",
    "            y_test=y_test_config,\n",
    "            dataset_metadata=config_dataset_metadata,\n",
    "            model_metadata=config_model_metadata,\n",
    "            class_names=list(class_names),\n",
    "            target_class=\"safety_car\",\n",
    "            save_results=True\n",
    "        )\n",
    "        \n",
    "        all_results[config['name']] = results\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# 8. Quick evaluation function for iterative testing\n",
    "def quick_eval(dataset, dataset_metadata, class_names, evaluator, model_name=\"logistic_regression\", \n",
    "               custom_weights=None, feature_subset=None):\n",
    "    \"\"\"Quick evaluation for rapid iteration\"\"\"\n",
    "    \n",
    "    X = dataset['X']\n",
    "    y = dataset['y']\n",
    "    X_aeon = X.transpose(0, 2, 1)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_data = X_aeon if feature_subset is None else X_aeon[:, feature_subset, :]\n",
    "    X_train_q, X_test_q, y_train_q, y_test_q = train_test_split(\n",
    "        X_data, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    if model_name == \"logistic_regression\":\n",
    "        model = Catch22Classifier(\n",
    "            estimator=LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=3000,\n",
    "                solver='saga',\n",
    "                penalty='l1',\n",
    "                C=0.1,\n",
    "                class_weight=custom_weights or class_weight\n",
    "            ),\n",
    "            outlier_norm=True,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Quick eval not implemented for {model_name}\")\n",
    "    \n",
    "    # Create metadata\n",
    "    feature_desc = f\"custom_subset_{len(feature_subset) if feature_subset else 'all'}_features\"\n",
    "    \n",
    "    from copy import deepcopy\n",
    "    quick_dataset_metadata = deepcopy(dataset_metadata)\n",
    "    quick_dataset_metadata.features_used = feature_desc\n",
    "    quick_dataset_metadata.n_features = X_data.shape[1]\n",
    "    \n",
    "    quick_model_metadata = create_model_metadata(\n",
    "        model_name=f\"{model_name}_quick\",\n",
    "        model=model,\n",
    "        class_weights=custom_weights\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    return evaluator.evaluate_model(\n",
    "        model=model,\n",
    "        model_name=f\"{model_name}_quick\",\n",
    "        X_train=X_train_q,\n",
    "        X_test=X_test_q,\n",
    "        y_train=y_train_q,\n",
    "        y_test=y_test_q,\n",
    "        dataset_metadata=quick_dataset_metadata,\n",
    "        model_metadata=quick_model_metadata,\n",
    "        class_names=list(class_names),\n",
    "        target_class=\"safety_car\",\n",
    "        save_results=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0edd2438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUICK ITERATION TESTING ===\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: LOGISTIC_REGRESSION_QUICK\n",
      "Evaluation ID: logistic_regression_quick_20250702_220343\n",
      "================================================================================\n",
      "Training model...\n",
      "Generating predictions...\n",
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.2566\n",
      "F1-Macro:    0.2455\n",
      "F1-Weighted: 0.2918\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       0.0935\n",
      "Recall:          1.0000\n",
      "F1-Score:        0.1711\n",
      "True Positives:    13\n",
      "False Negatives:    0 (missed safety_car events)\n",
      "False Positives:  126 (false safety_car alarms)\n",
      "True Negatives:   126\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=1.000, R=0.176, F1=0.300, N=238\n",
      "safety_car  : P=0.094, R=1.000, F1=0.171, N=13\n",
      "yellow      : P=0.155, R=0.929, F1=0.265, N=14\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green               42              125           71\n",
      "True_safety_car           0               13            0\n",
      "True_yellow               0                1           13\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/logistic_regression_quick_20250702_220343_complete.json\n",
      "  Summary:  evaluation_results/logistic_regression_quick_20250702_220343_summary.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seansica/Documents/Development/mids/capstone/datasci-210-2025-summer-formula1/sean/notebooks/.venv/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fixed example usage patterns at the bottom\n",
    "print(\"\\n=== QUICK ITERATION TESTING ===\")\n",
    "# Test with just speed feature\n",
    "speed_results = quick_eval(\n",
    "    dataset=dataset,\n",
    "    dataset_metadata=dataset_metadata,\n",
    "    class_names=class_names,\n",
    "    evaluator=evaluator,\n",
    "    feature_subset=[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e9b3254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING: LOGISTIC_REGRESSION_QUICK\n",
      "Evaluation ID: logistic_regression_quick_20250702_220404\n",
      "================================================================================\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seansica/Documents/Development/mids/capstone/datasci-210-2025-summer-formula1/sean/notebooks/.venv/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.7245\n",
      "F1-Macro:    0.5681\n",
      "F1-Weighted: 0.7843\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       0.1818\n",
      "Recall:          0.9231\n",
      "F1-Score:        0.3038\n",
      "True Positives:    12\n",
      "False Negatives:    1 (missed safety_car events)\n",
      "False Positives:   54 (false safety_car alarms)\n",
      "True Negatives:   198\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=0.994, R=0.702, F1=0.823, N=238\n",
      "safety_car  : P=0.182, R=0.923, F1=0.304, N=13\n",
      "yellow      : P=0.419, R=0.929, F1=0.578, N=14\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green              167               53           18\n",
      "True_safety_car           1               12            0\n",
      "True_yellow               0                1           13\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/logistic_regression_quick_20250702_220404_complete.json\n",
      "  Summary:  evaluation_results/logistic_regression_quick_20250702_220404_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Test with different class weights\n",
    "high_safety_weights = {\n",
    "    dataset['label_encoder'].class_to_idx['green']: 1.0,\n",
    "    dataset['label_encoder'].class_to_idx['red']: 50.0,         \n",
    "    dataset['label_encoder'].class_to_idx['safety_car']: 200.0,  # Even higher\n",
    "    dataset['label_encoder'].class_to_idx['unknown']: 50.0,      \n",
    "    dataset['label_encoder'].class_to_idx['vsc']: 100.0,\n",
    "    dataset['label_encoder'].class_to_idx['vsc_ending']: 100.0,\n",
    "    dataset['label_encoder'].class_to_idx['yellow']: 50.0\n",
    "}\n",
    "\n",
    "high_weight_results = quick_eval(\n",
    "    dataset=dataset,\n",
    "    dataset_metadata=dataset_metadata,\n",
    "    class_names=class_names,\n",
    "    evaluator=evaluator,\n",
    "    custom_weights=high_safety_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6527684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPREHENSIVE MODEL COMPARISON ===\n",
      "\n",
      "====================================================================================================\n",
      "EVALUATING MODEL: dummy_frequent\n",
      "====================================================================================================\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DUMMY_FREQUENT\n",
      "Evaluation ID: dummy_frequent_20250702_221117\n",
      "================================================================================\n",
      "Training model...\n",
      "Generating predictions...\n",
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.8981\n",
      "F1-Macro:    0.3154\n",
      "F1-Weighted: 0.8499\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       0.0000\n",
      "Recall:          0.0000\n",
      "F1-Score:        0.0000\n",
      "True Positives:     0\n",
      "False Negatives:   13 (missed safety_car events)\n",
      "False Positives:    0 (false safety_car alarms)\n",
      "True Negatives:   252\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=0.898, R=1.000, F1=0.946, N=238\n",
      "safety_car  : P=0.000, R=0.000, F1=0.000, N=13\n",
      "yellow      : P=0.000, R=0.000, F1=0.000, N=14\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green              238                0            0\n",
      "True_safety_car          13                0            0\n",
      "True_yellow              14                0            0\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/dummy_frequent_20250702_221117_complete.json\n",
      "  Summary:  evaluation_results/dummy_frequent_20250702_221117_summary.txt\n",
      "\n",
      "====================================================================================================\n",
      "EVALUATING MODEL: dummy_stratified\n",
      "====================================================================================================\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DUMMY_STRATIFIED\n",
      "Evaluation ID: dummy_stratified_20250702_221117\n",
      "================================================================================\n",
      "Training model...\n",
      "Generating predictions...\n",
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.8226\n",
      "F1-Macro:    0.3246\n",
      "F1-Weighted: 0.8141\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       0.0000\n",
      "Recall:          0.0000\n",
      "F1-Score:        0.0000\n",
      "True Positives:     0\n",
      "False Negatives:   13 (missed safety_car events)\n",
      "False Positives:    8 (false safety_car alarms)\n",
      "True Negatives:   244\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=0.893, R=0.912, F1=0.902, N=238\n",
      "safety_car  : P=0.000, R=0.000, F1=0.000, N=13\n",
      "yellow      : P=0.071, R=0.071, F1=0.071, N=14\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green              217                8           13\n",
      "True_safety_car          13                0            0\n",
      "True_yellow              13                0            1\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/dummy_stratified_20250702_221117_complete.json\n",
      "  Summary:  evaluation_results/dummy_stratified_20250702_221117_summary.txt\n",
      "\n",
      "====================================================================================================\n",
      "EVALUATING MODEL: logistic_regression\n",
      "====================================================================================================\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: LOGISTIC_REGRESSION\n",
      "Evaluation ID: logistic_regression_20250702_221117\n",
      "================================================================================\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seansica/Documents/Development/mids/capstone/datasci-210-2025-summer-formula1/sean/notebooks/.venv/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.8113\n",
      "F1-Macro:    0.6413\n",
      "F1-Weighted: 0.8477\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       0.2391\n",
      "Recall:          0.8462\n",
      "F1-Score:        0.3729\n",
      "True Positives:    11\n",
      "False Negatives:    2 (missed safety_car events)\n",
      "False Positives:   35 (false safety_car alarms)\n",
      "True Negatives:   217\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=0.985, R=0.803, F1=0.884, N=238\n",
      "safety_car  : P=0.239, R=0.846, F1=0.373, N=13\n",
      "yellow      : P=0.520, R=0.929, F1=0.667, N=14\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green              191               35           12\n",
      "True_safety_car           2               11            0\n",
      "True_yellow               1                0           13\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/logistic_regression_20250702_221117_complete.json\n",
      "  Summary:  evaluation_results/logistic_regression_20250702_221117_summary.txt\n",
      "\n",
      "====================================================================================================\n",
      "EVALUATING MODEL: random_forest\n",
      "====================================================================================================\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: RANDOM_FOREST\n",
      "Evaluation ID: random_forest_20250702_221131\n",
      "================================================================================\n",
      "Training model...\n",
      "Generating predictions...\n",
      "\n",
      "📊 OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:    0.9509\n",
      "F1-Macro:    0.8331\n",
      "F1-Weighted: 0.9513\n",
      "\n",
      "🎯 TARGET CLASS ANALYSIS: SAFETY_CAR\n",
      "==================================================\n",
      "Precision:       1.0000\n",
      "Recall:          0.6154\n",
      "F1-Score:        0.7619\n",
      "True Positives:     8\n",
      "False Negatives:    5 (missed safety_car events)\n",
      "False Positives:    0 (false safety_car alarms)\n",
      "True Negatives:   252\n",
      "\n",
      "📈 PER-CLASS PERFORMANCE\n",
      "==================================================\n",
      "green       : P=0.975, R=0.971, F1=0.973, N=238\n",
      "safety_car  : P=1.000, R=0.615, F1=0.762, N=13\n",
      "yellow      : P=0.650, R=0.929, F1=0.765, N=14\n",
      "\n",
      "🔍 CONFUSION MATRIX\n",
      "==================================================\n",
      "                 Pred_green  Pred_safety_car  Pred_yellow\n",
      "True_green              231                0            7\n",
      "True_safety_car           5                8            0\n",
      "True_yellow               1                0           13\n",
      "\n",
      "💾 Results saved:\n",
      "  Complete: evaluation_results/random_forest_20250702_221131_complete.json\n",
      "  Summary:  evaluation_results/random_forest_20250702_221131_summary.txt\n",
      "\n",
      "📊 Model comparison saved: evaluation_results/model_comparison_20250702_221134_comparison.json\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive model comparison\n",
    "print(\"\\n=== COMPREHENSIVE MODEL COMPARISON ===\")\n",
    "all_model_results = run_comprehensive_model_comparison(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    dataset_metadata=dataset_metadata,\n",
    "    class_names=class_names,\n",
    "    evaluator=evaluator,\n",
    "    class_weight=class_weight\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
