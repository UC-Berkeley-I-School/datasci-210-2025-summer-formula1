{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "F1 Safety Car Prediction ETL Pipeline\n",
    "\n",
    "This module provides utilities for processing Formula 1 data through various stages:\n",
    "1. Raw fastf1 data extraction\n",
    "2. Data aggregation across sessions/races\n",
    "3. Label encoding for track status\n",
    "4. Time series sequence generation\n",
    "5. Feature engineering enhancement\n",
    "\"\"\"\n",
    "\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class SessionConfig:\n",
    "    \"\"\"Configuration for a single F1 session\"\"\"\n",
    "    year: int\n",
    "    race: str  # e.g., 'Saudi Arabian Grand Prix'\n",
    "    session_type: str  # 'R' for race, 'Q' for qualifying, etc.\n",
    "\n",
    "@dataclass \n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data processing\"\"\"\n",
    "    sessions: List[SessionConfig]\n",
    "    drivers: Optional[List[str]] = None  # None = all drivers\n",
    "    telemetry_frequency: Union[str, int] = 'original'\n",
    "    include_weather: bool = True\n",
    "    cache_dir: Optional[str] = None\n",
    "\n",
    "class RawDataExtractor:\n",
    "    \"\"\"Handles extraction of raw fastf1 data\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[str] = None):\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else None\n",
    "        if self.cache_dir:\n",
    "            self.cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def extract_session(self, config: SessionConfig) -> Dict[str, Any]:\n",
    "        \"\"\"Extract all data for a single session\"\"\"\n",
    "        print(f\"Loading session: {config.year} {config.race} {config.session_type}\")\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = f\"{config.year}_{config.race}_{config.session_type}\".replace(' ', '_')\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\" if self.cache_dir else None\n",
    "        \n",
    "        if cache_file and cache_file.exists():\n",
    "            print(f\"Loading from cache: {cache_file}\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        # Load session\n",
    "        session = fastf1.get_session(config.year, config.race, config.session_type)\n",
    "        session.load()\n",
    "        \n",
    "        # Create mapping from driver numbers to abbreviations\n",
    "        driver_mapping = {}\n",
    "        for driver_number in session.drivers:\n",
    "            driver_info = session.get_driver(driver_number)\n",
    "            driver_mapping[driver_number] = driver_info['Abbreviation']\n",
    "        \n",
    "        # Extract all relevant data\n",
    "        session_data = {\n",
    "            'session_info': {\n",
    "                'year': config.year,\n",
    "                'race': config.race,\n",
    "                'session_type': config.session_type,\n",
    "                'event_name': session.event.EventName,\n",
    "                'event_date': session.event.EventDate,\n",
    "                'session_start': session.session_start_time,\n",
    "                't0_date': session.t0_date  # Add this for time alignment\n",
    "            },\n",
    "            'laps': session.laps,\n",
    "            'weather': session.weather_data,\n",
    "            'track_status': session.track_status,\n",
    "            'car_data': {},\n",
    "            'pos_data': {},\n",
    "            'drivers': list(session.drivers),  # Driver numbers as strings\n",
    "            'driver_mapping': driver_mapping  # Add mapping for later use\n",
    "        }\n",
    "        \n",
    "        # Extract telemetry for each driver using abbreviations\n",
    "        for driver_number in session_data['drivers']:\n",
    "            try:\n",
    "                session_data['car_data'][driver_number] = session.car_data[driver_number]\n",
    "                session_data['pos_data'][driver_number] = session.pos_data[driver_number]\n",
    "            except Exception as e:\n",
    "                abbreviation = driver_mapping.get(driver_number, driver_number)\n",
    "                print(f\"Warning: Could not extract telemetry for driver ({abbreviation}, {driver_number}): {e}\")\n",
    "        \n",
    "        # Cache the result\n",
    "        if cache_file:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(session_data, f)\n",
    "        \n",
    "        return session_data\n",
    "\n",
    "class DataAggregator:\n",
    "    \"\"\"Aggregates raw data across multiple sessions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.aggregated_data = defaultdict(list)\n",
    "    \n",
    "    def aggregate_lap_data(self, sessions_data: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate lap data across sessions with session identifiers\"\"\"\n",
    "        all_laps = []\n",
    "        \n",
    "        for session_data in sessions_data:\n",
    "            laps = session_data['laps'].copy()\n",
    "            \n",
    "            # Add session identifiers\n",
    "            laps['SessionYear'] = session_data['session_info']['year']\n",
    "            laps['SessionRace'] = session_data['session_info']['race']\n",
    "            laps['SessionType'] = session_data['session_info']['session_type']\n",
    "            laps['SessionId'] = f\"{session_data['session_info']['year']}_{session_data['session_info']['race']}_{session_data['session_info']['session_type']}\"\n",
    "            \n",
    "            all_laps.append(laps)\n",
    "        \n",
    "        return pd.concat(all_laps, ignore_index=True)\n",
    "    \n",
    "    def aggregate_telemetry_data(self, sessions_data: List[Dict[str, Any]], \n",
    "                            drivers: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate telemetry data across sessions with track status alignment\"\"\"\n",
    "        all_telemetry = []\n",
    "        \n",
    "        for session_data in sessions_data:\n",
    "            session_telemetry = self._merge_session_telemetry(session_data, drivers)\n",
    "            \n",
    "            # Add session identifiers\n",
    "            session_telemetry['SessionYear'] = session_data['session_info']['year']\n",
    "            session_telemetry['SessionRace'] = session_data['session_info']['race']\n",
    "            session_telemetry['SessionType'] = session_data['session_info']['session_type']\n",
    "            session_telemetry['SessionId'] = f\"{session_data['session_info']['year']}_{session_data['session_info']['race']}_{session_data['session_info']['session_type']}\"\n",
    "            \n",
    "            # Align track status with telemetry timestamps\n",
    "            track_status = session_data.get('track_status', pd.DataFrame())\n",
    "            t0_date = session_data['session_info']['t0_date']  # Get t0_date from session info\n",
    "            session_telemetry = self._align_track_status(session_telemetry, track_status, t0_date)\n",
    "            \n",
    "            all_telemetry.append(session_telemetry)\n",
    "        \n",
    "        return pd.concat(all_telemetry, ignore_index=True)\n",
    "\n",
    "    def _align_track_status(self, telemetry: pd.DataFrame, track_status: pd.DataFrame, t0_date) -> pd.DataFrame:\n",
    "        \"\"\"Align track status with telemetry timestamps using forward fill\"\"\"\n",
    "        if track_status is None or track_status.empty or telemetry.empty:\n",
    "            # If no track status data or no telemetry data, assume track clear\n",
    "            if not telemetry.empty:\n",
    "                telemetry['TrackStatus'] = '1'\n",
    "                telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        # Check if we have the required columns in telemetry\n",
    "        if 'Date' not in telemetry.columns:\n",
    "            print(\"Warning: No Date column in telemetry data, skipping track status alignment\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        # Track status data structure from FastF1\n",
    "        track_status = track_status.copy()\n",
    "        \n",
    "        # Check if track status has the expected structure\n",
    "        if 'Time' not in track_status.columns or 'Status' not in track_status.columns:\n",
    "            print(\"Warning: Track status data missing required columns, using default\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        try:\n",
    "            # Convert track status SessionTime to absolute Date using t0_date\n",
    "            track_status_with_date = track_status.copy()\n",
    "            track_status_with_date['Date'] = t0_date + track_status_with_date['Time']\n",
    "            \n",
    "            # Get available columns from track status\n",
    "            status_cols = ['Date', 'Status']\n",
    "            if 'Message' in track_status_with_date.columns:\n",
    "                status_cols.append('Message')\n",
    "            \n",
    "            # Forward fill track status for all telemetry timestamps\n",
    "            telemetry_with_status = pd.merge_asof(\n",
    "                telemetry.sort_values('Date'),\n",
    "                track_status_with_date[status_cols].sort_values('Date'),\n",
    "                on='Date',\n",
    "                direction='backward'\n",
    "            ).fillna({'Status': '1'})\n",
    "            \n",
    "            if 'Message' not in telemetry_with_status.columns:\n",
    "                telemetry_with_status['Message'] = 'AllClear'\n",
    "            else:\n",
    "                telemetry_with_status['Message'] = telemetry_with_status['Message'].fillna('AllClear')\n",
    "            \n",
    "            telemetry_with_status = telemetry_with_status.rename(columns={\n",
    "                'Status': 'TrackStatus',\n",
    "                'Message': 'TrackStatusMessage'\n",
    "            })\n",
    "            \n",
    "            return telemetry_with_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to align track status: {e}\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "    \n",
    "    def _merge_session_telemetry(self, session_data: Dict[str, Any], \n",
    "                               drivers: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Merge car and position data for a single session\"\"\"\n",
    "        session_drivers = drivers if drivers else session_data['drivers']\n",
    "        session_telemetry = []\n",
    "        \n",
    "        for driver_number in session_drivers:\n",
    "            if driver_number not in session_data['car_data']:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Get car and position data\n",
    "                car_data = session_data['car_data'][driver_number]\n",
    "                pos_data = session_data['pos_data'][driver_number]\n",
    "                \n",
    "                # Merge using fastf1's native method\n",
    "                merged = car_data.merge_channels(pos_data, frequency='original')\n",
    "                \n",
    "                # Add computed channels\n",
    "                merged = merged.add_distance().add_differential_distance()\n",
    "                \n",
    "                # Add driver identifier (use driver number)\n",
    "                merged['Driver'] = driver_number\n",
    "                \n",
    "                session_telemetry.append(merged)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not merge telemetry for driver {driver_number}: {e}\")\n",
    "        \n",
    "        if session_telemetry:\n",
    "            result = pd.concat(session_telemetry, ignore_index=True)\n",
    "            # Ensure we have the required columns\n",
    "            if 'Date' not in result.columns and 'Time' in result.columns:\n",
    "                result = result.rename(columns={'Time': 'Date'})\n",
    "            elif 'Date' not in result.columns and hasattr(result, 'index') and hasattr(result.index, 'name'):\n",
    "                # If Date is the index, reset it\n",
    "                result = result.reset_index()\n",
    "                if 'index' in result.columns:\n",
    "                    result = result.rename(columns={'index': 'Date'})\n",
    "            return result\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "class DriverLabelEncoder:\n",
    "    \"\"\"Encodes driver identifiers for consistency\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        self.driver_to_number = {}  # Maps abbreviations to driver numbers\n",
    "        self.number_to_driver = {}  # Maps driver numbers to abbreviations\n",
    "    \n",
    "    def fit_session(self, session) -> 'DriverLabelEncoder':\n",
    "        \"\"\"Fit the encoder using session driver data\"\"\"\n",
    "        driver_numbers = session.drivers\n",
    "        \n",
    "        for driver_number in driver_numbers:\n",
    "            driver_info = session.get_driver(driver_number)\n",
    "            abbreviation = driver_info['Abbreviation']\n",
    "            \n",
    "            self.driver_to_number[abbreviation] = driver_number\n",
    "            self.number_to_driver[driver_number] = abbreviation\n",
    "        \n",
    "        # Fit encoder on abbreviations for consistent encoding\n",
    "        abbreviations = list(self.driver_to_number.keys())\n",
    "        self.label_encoder.fit(abbreviations)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform_driver_to_number(self, drivers):\n",
    "        \"\"\"Transform driver abbreviations to driver numbers\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Encoder not fitted\")\n",
    "        return [self.driver_to_number[driver] for driver in drivers]\n",
    "    \n",
    "    def transform_number_to_driver(self, numbers):\n",
    "        \"\"\"Transform driver numbers to abbreviations\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Encoder not fitted\")\n",
    "        return [self.number_to_driver[number] for number in numbers]\n",
    "\n",
    "class TrackStatusLabelEncoder:\n",
    "    \"\"\"Encodes track status labels for safety car prediction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        self.track_status_mapping = {\n",
    "            '1': 'green',      # Track clear\n",
    "            '2': 'yellow',     # Yellow flag\n",
    "            '4': 'safety_car', # Safety car deployed\n",
    "            '5': 'red',        # Red flag\n",
    "            '6': 'vsc',        # Virtual safety car\n",
    "            '7': 'vsc_ending'  # VSC ending\n",
    "        }\n",
    "    \n",
    "    def fit(self, track_status_data: pd.Series) -> 'TrackStatusLabelEncoder':\n",
    "        \"\"\"Fit the label encoder on track status data\"\"\"\n",
    "        # Map numeric codes to readable labels\n",
    "        mapped_labels = track_status_data.map(self.track_status_mapping).fillna('unknown')\n",
    "        \n",
    "        self.label_encoder.fit(mapped_labels)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, track_status_data: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Transform track status data to encoded labels\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"LabelEncoder must be fitted before transform\")\n",
    "        \n",
    "        mapped_labels = track_status_data.map(self.track_status_mapping).fillna('unknown')\n",
    "        return self.label_encoder.transform(mapped_labels)\n",
    "    \n",
    "    def fit_transform(self, track_status_data: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(track_status_data).transform(track_status_data)\n",
    "    \n",
    "    def inverse_transform(self, encoded_labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert encoded labels back to readable format\"\"\"\n",
    "        return self.label_encoder.inverse_transform(encoded_labels)\n",
    "    \n",
    "    def get_classes(self) -> np.ndarray:\n",
    "        \"\"\"Get the class labels\"\"\"\n",
    "        return self.label_encoder.classes_\n",
    "\n",
    "class TimeSeriesGenerator:\n",
    "    \"\"\"Generates sliding window time series sequences from telemetry data\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int, step_size: int = 1, \n",
    "                 features: Optional[List[str]] = None,\n",
    "                 prediction_horizon: int = 1,\n",
    "                 padding_strategy: str = 'none',  # 'none', 'zero', 'repeat_last'\n",
    "                 truncation_strategy: str = 'none'):  # 'none', 'trim_start', 'trim_end'\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.padding_strategy = padding_strategy\n",
    "        self.truncation_strategy = truncation_strategy\n",
    "        self.features = features or ['Speed', 'RPM', 'nGear', 'Throttle', 'Brake', \n",
    "                                   'X', 'Y', 'Distance', 'DifferentialDistance']\n",
    "    \n",
    "    def generate_sequences(self, telemetry_data: pd.DataFrame, \n",
    "                         group_by: List[str] = None) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Generate sliding window sequences from telemetry data\n",
    "        \n",
    "        Returns:\n",
    "            X: sequences of shape (n_sequences, window_size, n_features)\n",
    "            y: labels of shape (n_sequences,) - track status at prediction horizon\n",
    "            metadata: list of dicts with sequence metadata\n",
    "        \"\"\"\n",
    "        if group_by is None:\n",
    "            group_by = ['SessionId', 'Driver']\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        # Group telemetry by session and driver\n",
    "        for group_keys, group_data in telemetry_data.groupby(group_by):\n",
    "            group_sequences, group_labels, group_metadata = self._generate_group_sequences(\n",
    "                group_data, group_keys, group_by\n",
    "            )\n",
    "            sequences.extend(group_sequences)\n",
    "            labels.extend(group_labels)\n",
    "            metadata.extend(group_metadata)\n",
    "        \n",
    "        # Apply padding/truncation if specified\n",
    "        if sequences:\n",
    "            sequences = self._apply_length_adjustments(sequences)\n",
    "        \n",
    "        return np.array(sequences), np.array(labels), metadata\n",
    "    \n",
    "    def _generate_group_sequences(self, group_data: pd.DataFrame, \n",
    "                                group_keys: Tuple, group_by: List[str]) -> Tuple[List, List, List]:\n",
    "        \"\"\"Generate sequences for a single group (e.g., driver in session)\"\"\"\n",
    "        # Sort by time to ensure proper sequence order\n",
    "        group_data = group_data.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        # Extract feature data\n",
    "        available_features = [f for f in self.features if f in group_data.columns]\n",
    "        if len(available_features) != len(self.features):\n",
    "            missing = set(self.features) - set(available_features)\n",
    "            print(f\"Warning: Missing features {missing}, using {available_features}\")\n",
    "        \n",
    "        feature_data = group_data[available_features].values\n",
    "        \n",
    "        # Generate sliding windows\n",
    "        max_start_idx = len(feature_data) - self.window_size - self.prediction_horizon + 1\n",
    "        for i in range(0, max_start_idx, self.step_size):\n",
    "            # Extract sequence\n",
    "            sequence = feature_data[i:i + self.window_size]\n",
    "            \n",
    "            # Label is the track status at prediction horizon after the sequence\n",
    "            label_idx = i + self.window_size + self.prediction_horizon - 1\n",
    "            if label_idx < len(group_data):\n",
    "                label = group_data.iloc[label_idx]['TrackStatus']\n",
    "            else:\n",
    "                continue  # Skip if we don't have future data\n",
    "            \n",
    "            # Metadata for this sequence\n",
    "            seq_metadata = {\n",
    "                'start_time': group_data.iloc[i]['Date'],\n",
    "                'end_time': group_data.iloc[i + self.window_size - 1]['Date'],\n",
    "                'prediction_time': group_data.iloc[label_idx]['Date'],\n",
    "                'sequence_length': self.window_size,\n",
    "                'prediction_horizon': self.prediction_horizon,\n",
    "                'features_used': available_features\n",
    "            }\n",
    "            \n",
    "            # Add group identifiers to metadata\n",
    "            for j, key in enumerate(group_by):\n",
    "                seq_metadata[key] = group_keys[j] if isinstance(group_keys, tuple) else group_keys\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "            metadata.append(seq_metadata)\n",
    "        \n",
    "        return sequences, labels, metadata\n",
    "    \n",
    "    def _apply_length_adjustments(self, sequences: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        \"\"\"Apply padding or truncation strategies to handle variable length sequences\"\"\"\n",
    "        if not sequences:\n",
    "            return sequences\n",
    "        \n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        target_length = self.window_size\n",
    "        \n",
    "        adjusted_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) == target_length:\n",
    "                adjusted_sequences.append(seq)\n",
    "            elif len(seq) < target_length:\n",
    "                # Sequence is too short - apply padding\n",
    "                adjusted_sequences.append(self._pad_sequence(seq, target_length))\n",
    "            else:\n",
    "                # Sequence is too long - apply truncation\n",
    "                adjusted_sequences.append(self._truncate_sequence(seq, target_length))\n",
    "        \n",
    "        return adjusted_sequences\n",
    "    \n",
    "    def _pad_sequence(self, sequence: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"Pad sequence to target length\"\"\"\n",
    "        if self.padding_strategy == 'none':\n",
    "            return sequence\n",
    "        elif self.padding_strategy == 'zero':\n",
    "            padding = np.zeros((target_length - len(sequence), sequence.shape[1]))\n",
    "            return np.vstack([sequence, padding])\n",
    "        elif self.padding_strategy == 'repeat_last':\n",
    "            if len(sequence) > 0:\n",
    "                last_row = np.tile(sequence[-1:], (target_length - len(sequence), 1))\n",
    "                return np.vstack([sequence, last_row])\n",
    "            else:\n",
    "                return np.zeros((target_length, sequence.shape[1] if len(sequence.shape) > 1 else 1))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown padding strategy: {self.padding_strategy}\")\n",
    "    \n",
    "    def _truncate_sequence(self, sequence: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"Truncate sequence to target length\"\"\"\n",
    "        if self.truncation_strategy == 'none':\n",
    "            return sequence\n",
    "        elif self.truncation_strategy == 'trim_start':\n",
    "            return sequence[-target_length:]\n",
    "        elif self.truncation_strategy == 'trim_end':\n",
    "            return sequence[:target_length]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown truncation strategy: {self.truncation_strategy}\")\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Applies feature engineering to time series data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transformers = {}\n",
    "        self.is_fitted = {}\n",
    "    \n",
    "    def apply_basic_features(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply basic statistical features (mean, std, min, max per sequence)\"\"\"\n",
    "        n_sequences, seq_length, n_features = X.shape\n",
    "        \n",
    "        # Calculate statistical features for each sequence\n",
    "        features = []\n",
    "        for i in range(n_sequences):\n",
    "            seq_features = []\n",
    "            for j in range(n_features):\n",
    "                feature_series = X[i, :, j]\n",
    "                seq_features.extend([\n",
    "                    np.mean(feature_series),\n",
    "                    np.std(feature_series),\n",
    "                    np.min(feature_series),\n",
    "                    np.max(feature_series),\n",
    "                    np.median(feature_series)\n",
    "                ])\n",
    "            features.append(seq_features)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def apply_catch22(self, X: np.ndarray, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Apply Catch22 feature extraction (placeholder for future implementation)\"\"\"\n",
    "        # Note: This is a placeholder - actual Catch22 implementation will be added later\n",
    "        # For now, just return basic statistical features\n",
    "        print(\"Warning: Using basic features instead of Catch22 (not yet implemented)\")\n",
    "        return self.apply_basic_features(X)\n",
    "    \n",
    "    def normalize_sequences(self, X: np.ndarray, method: str = 'standard', \n",
    "                           fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Normalize time series sequences\"\"\"\n",
    "        transformer_key = f\"{method}_normalizer\"\n",
    "        \n",
    "        if method == 'standard':\n",
    "            # Z-score normalization per feature across all sequences\n",
    "            if fit or transformer_key not in self.transformers:\n",
    "                # Calculate mean and std across all sequences and time steps\n",
    "                means = np.mean(X, axis=(0, 1), keepdims=True)\n",
    "                stds = np.std(X, axis=(0, 1), keepdims=True)\n",
    "                stds = np.where(stds == 0, 1, stds)  # Avoid division by zero\n",
    "                \n",
    "                self.transformers[transformer_key] = {'means': means, 'stds': stds}\n",
    "                self.is_fitted[transformer_key] = True\n",
    "            \n",
    "            params = self.transformers[transformer_key]\n",
    "            return (X - params['means']) / params['stds']\n",
    "            \n",
    "        elif method == 'minmax':\n",
    "            # Min-max normalization per feature\n",
    "            if fit or transformer_key not in self.transformers:\n",
    "                mins = np.min(X, axis=(0, 1), keepdims=True)\n",
    "                maxs = np.max(X, axis=(0, 1), keepdims=True)\n",
    "                ranges = maxs - mins\n",
    "                ranges = np.where(ranges == 0, 1, ranges)  # Avoid division by zero\n",
    "                \n",
    "                self.transformers[transformer_key] = {'mins': mins, 'ranges': ranges}\n",
    "                self.is_fitted[transformer_key] = True\n",
    "            \n",
    "            params = self.transformers[transformer_key]\n",
    "            return (X - params['mins']) / params['ranges']\n",
    "            \n",
    "        elif method == 'per_sequence':\n",
    "            # Normalize each sequence independently\n",
    "            normalized = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                seq = X[i]\n",
    "                seq_mean = np.mean(seq, axis=0, keepdims=True)\n",
    "                seq_std = np.std(seq, axis=0, keepdims=True)\n",
    "                seq_std = np.where(seq_std == 0, 1, seq_std)\n",
    "                normalized[i] = (seq - seq_mean) / seq_std\n",
    "            return normalized\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "    \n",
    "    def handle_missing_values(self, X: np.ndarray, strategy: str = 'forward_fill') -> np.ndarray:\n",
    "        \"\"\"Handle missing values in time series sequences\"\"\"\n",
    "        X_filled = X.copy()\n",
    "        \n",
    "        if strategy == 'forward_fill':\n",
    "            # Forward fill missing values within each sequence\n",
    "            for i in range(X.shape[0]):  # For each sequence\n",
    "                for j in range(X.shape[2]):  # For each feature\n",
    "                    series = X_filled[i, :, j]\n",
    "                    mask = np.isnan(series)\n",
    "                    if mask.any():\n",
    "                        # Forward fill\n",
    "                        valid_indices = np.where(~mask)[0]\n",
    "                        if len(valid_indices) > 0:\n",
    "                            # Use numpy interp for forward fill approximation\n",
    "                            filled_values = np.full_like(series, np.nan)\n",
    "                            filled_values[valid_indices] = series[valid_indices]\n",
    "                            # Forward fill manually\n",
    "                            last_valid = np.nan\n",
    "                            for k in range(len(filled_values)):\n",
    "                                if not np.isnan(filled_values[k]):\n",
    "                                    last_valid = filled_values[k]\n",
    "                                elif not np.isnan(last_valid):\n",
    "                                    filled_values[k] = last_valid\n",
    "                            series[:] = filled_values\n",
    "                        X_filled[i, :, j] = series\n",
    "                        \n",
    "        elif strategy == 'mean_fill':\n",
    "            # Fill with feature mean across all sequences\n",
    "            for j in range(X.shape[2]):\n",
    "                feature_data = X[:, :, j]\n",
    "                feature_mean = np.nanmean(feature_data)\n",
    "                X_filled[:, :, j] = np.where(np.isnan(feature_data), feature_mean, feature_data)\n",
    "                \n",
    "        elif strategy == 'zero_fill':\n",
    "            # Fill with zeros\n",
    "            X_filled = np.where(np.isnan(X_filled), 0, X_filled)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown missing value strategy: {strategy}\")\n",
    "        \n",
    "        return X_filled\n",
    "\n",
    "# Example usage pipeline\n",
    "def create_safety_car_dataset(config: DataConfig, \n",
    "                             window_size: int = 100,\n",
    "                             prediction_horizon: int = 10,\n",
    "                             padding_strategy: str = 'none',\n",
    "                             truncation_strategy: str = 'none') -> Dict[str, Any]:\n",
    "    \"\"\"Complete ETL pipeline for safety car prediction dataset\"\"\"\n",
    "    \n",
    "    # Step 1: Extract raw data\n",
    "    extractor = RawDataExtractor(config.cache_dir)\n",
    "    sessions_data = [extractor.extract_session(session_config) \n",
    "                    for session_config in config.sessions]\n",
    "    \n",
    "    # Step 2: Aggregate data with track status alignment\n",
    "    aggregator = DataAggregator()\n",
    "    telemetry_data = aggregator.aggregate_telemetry_data(sessions_data, config.drivers)\n",
    "    \n",
    "    # Step 3: Encode labels\n",
    "    label_encoder = TrackStatusLabelEncoder()\n",
    "    if 'TrackStatus' in telemetry_data.columns:\n",
    "        encoded_labels = label_encoder.fit_transform(telemetry_data['TrackStatus'])\n",
    "        telemetry_data['TrackStatusEncoded'] = encoded_labels\n",
    "    else:\n",
    "        print(\"Warning: No track status data found\")\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Generate time series sequences with prediction horizon\n",
    "    ts_generator = TimeSeriesGenerator(\n",
    "        window_size=window_size, \n",
    "        step_size=window_size // 2,  # 50% overlap\n",
    "        prediction_horizon=prediction_horizon,\n",
    "        padding_strategy=padding_strategy,\n",
    "        truncation_strategy=truncation_strategy\n",
    "    )\n",
    "    X, y, metadata = ts_generator.generate_sequences(telemetry_data)\n",
    "    \n",
    "    # Step 5: Encode the prediction labels\n",
    "    y_encoded = label_encoder.transform(pd.Series(y))\n",
    "    \n",
    "    # Step 6: Apply basic feature engineering\n",
    "    engineer = FeatureEngineer()\n",
    "    \n",
    "    # Handle missing values first\n",
    "    X_clean = engineer.handle_missing_values(X)\n",
    "    \n",
    "    # Normalize sequences\n",
    "    X_normalized = engineer.normalize_sequences(X_clean, method='standard')\n",
    "    \n",
    "    # Calculate class distribution for analysis\n",
    "    unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "    class_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    return {\n",
    "        'X': X_normalized,\n",
    "        'y': y_encoded,\n",
    "        'y_raw': y,\n",
    "        'metadata': metadata,\n",
    "        'label_encoder': label_encoder,\n",
    "        'raw_telemetry': telemetry_data,\n",
    "        'class_distribution': class_distribution,\n",
    "        'feature_engineer': engineer,\n",
    "        'config': {\n",
    "            'window_size': window_size,\n",
    "            'prediction_horizon': prediction_horizon,\n",
    "            'padding_strategy': padding_strategy,\n",
    "            'truncation_strategy': truncation_strategy,\n",
    "            'n_sequences': len(X_normalized),\n",
    "            'n_features': X_normalized.shape[2] if len(X_normalized) > 0 else 0\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c758eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "F1 Safety Car Prediction ETL Pipeline - Fixed Version\n",
    "\n",
    "Key fixes:\n",
    "1. Proper handling of mixed data types from FastF1\n",
    "2. Feature validation and type checking\n",
    "3. Preprocessing order following aeon principles\n",
    "4. Avoid redundant preprocessing steps\n",
    "\"\"\"\n",
    "\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "@dataclass\n",
    "class SessionConfig:\n",
    "    \"\"\"Configuration for a single F1 session\"\"\"\n",
    "    year: int\n",
    "    race: str\n",
    "    session_type: str\n",
    "\n",
    "@dataclass \n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data processing\"\"\"\n",
    "    sessions: List[SessionConfig]\n",
    "    drivers: Optional[List[str]] = None\n",
    "    telemetry_frequency: Union[str, int] = 'original'\n",
    "    include_weather: bool = True\n",
    "    cache_dir: Optional[str] = None\n",
    "\n",
    "class RawDataExtractor:\n",
    "    \"\"\"Handles extraction of raw fastf1 data\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[str] = None):\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else None\n",
    "        if self.cache_dir:\n",
    "            self.cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def extract_session(self, config: SessionConfig) -> Dict[str, Any]:\n",
    "        \"\"\"Extract all data for a single session\"\"\"\n",
    "        print(f\"Loading session: {config.year} {config.race} {config.session_type}\")\n",
    "        \n",
    "        cache_key = f\"{config.year}_{config.race}_{config.session_type}\".replace(' ', '_')\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\" if self.cache_dir else None\n",
    "        \n",
    "        if cache_file and cache_file.exists():\n",
    "            print(f\"Loading from cache: {cache_file}\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        session = fastf1.get_session(config.year, config.race, config.session_type)\n",
    "        session.load()\n",
    "        \n",
    "        driver_mapping = {}\n",
    "        for driver_number in session.drivers:\n",
    "            driver_info = session.get_driver(driver_number)\n",
    "            driver_mapping[driver_number] = driver_info['Abbreviation']\n",
    "        \n",
    "        session_data = {\n",
    "            'session_info': {\n",
    "                'year': config.year,\n",
    "                'race': config.race,\n",
    "                'session_type': config.session_type,\n",
    "                'event_name': session.event.EventName,\n",
    "                'event_date': session.event.EventDate,\n",
    "                'session_start': session.session_start_time,\n",
    "                't0_date': session.t0_date\n",
    "            },\n",
    "            'laps': session.laps,\n",
    "            'weather': session.weather_data,\n",
    "            'track_status': session.track_status,\n",
    "            'car_data': {},\n",
    "            'pos_data': {},\n",
    "            'drivers': list(session.drivers),\n",
    "            'driver_mapping': driver_mapping\n",
    "        }\n",
    "        \n",
    "        for driver_number in session_data['drivers']:\n",
    "            try:\n",
    "                session_data['car_data'][driver_number] = session.car_data[driver_number]\n",
    "                session_data['pos_data'][driver_number] = session.pos_data[driver_number]\n",
    "            except Exception as e:\n",
    "                abbreviation = driver_mapping.get(driver_number, driver_number)\n",
    "                print(f\"Warning: Could not extract telemetry for driver ({abbreviation}, {driver_number}): {e}\")\n",
    "        \n",
    "        if cache_file:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(session_data, f)\n",
    "        \n",
    "        return session_data\n",
    "\n",
    "class DataAggregator:\n",
    "    \"\"\"Aggregates raw data across multiple sessions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.aggregated_data = defaultdict(list)\n",
    "    \n",
    "    def aggregate_telemetry_data(self, sessions_data: List[Dict[str, Any]], \n",
    "                            drivers: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate telemetry data across sessions with track status alignment\"\"\"\n",
    "        all_telemetry = []\n",
    "        \n",
    "        for session_data in sessions_data:\n",
    "            session_telemetry = self._merge_session_telemetry(session_data, drivers)\n",
    "            \n",
    "            session_telemetry['SessionYear'] = session_data['session_info']['year']\n",
    "            session_telemetry['SessionRace'] = session_data['session_info']['race']\n",
    "            session_telemetry['SessionType'] = session_data['session_info']['session_type']\n",
    "            session_telemetry['SessionId'] = f\"{session_data['session_info']['year']}_{session_data['session_info']['race']}_{session_data['session_info']['session_type']}\"\n",
    "            \n",
    "            track_status = session_data.get('track_status', pd.DataFrame())\n",
    "            t0_date = session_data['session_info']['t0_date']\n",
    "            session_telemetry = self._align_track_status(session_telemetry, track_status, t0_date)\n",
    "            \n",
    "            all_telemetry.append(session_telemetry)\n",
    "        \n",
    "        return pd.concat(all_telemetry, ignore_index=True)\n",
    "\n",
    "    def _align_track_status(self, telemetry: pd.DataFrame, track_status: pd.DataFrame, t0_date) -> pd.DataFrame:\n",
    "        \"\"\"Align track status with telemetry timestamps using forward fill\"\"\"\n",
    "        if track_status is None or track_status.empty or telemetry.empty:\n",
    "            if not telemetry.empty:\n",
    "                telemetry['TrackStatus'] = '1'\n",
    "                telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        if 'Date' not in telemetry.columns:\n",
    "            print(\"Warning: No Date column in telemetry data, skipping track status alignment\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        if 'Time' not in track_status.columns or 'Status' not in track_status.columns:\n",
    "            print(\"Warning: Track status data missing required columns, using default\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        try:\n",
    "            track_status_with_date = track_status.copy()\n",
    "            track_status_with_date['Date'] = t0_date + track_status_with_date['Time']\n",
    "            \n",
    "            status_cols = ['Date', 'Status']\n",
    "            if 'Message' in track_status_with_date.columns:\n",
    "                status_cols.append('Message')\n",
    "            \n",
    "            telemetry_with_status = pd.merge_asof(\n",
    "                telemetry.sort_values('Date'),\n",
    "                track_status_with_date[status_cols].sort_values('Date'),\n",
    "                on='Date',\n",
    "                direction='backward'\n",
    "            ).fillna({'Status': '1'})\n",
    "            \n",
    "            if 'Message' not in telemetry_with_status.columns:\n",
    "                telemetry_with_status['Message'] = 'AllClear'\n",
    "            else:\n",
    "                telemetry_with_status['Message'] = telemetry_with_status['Message'].fillna('AllClear')\n",
    "            \n",
    "            telemetry_with_status = telemetry_with_status.rename(columns={\n",
    "                'Status': 'TrackStatus',\n",
    "                'Message': 'TrackStatusMessage'\n",
    "            })\n",
    "            \n",
    "            return telemetry_with_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to align track status: {e}\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "    \n",
    "    def _merge_session_telemetry(self, session_data: Dict[str, Any], \n",
    "                               drivers: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Merge car and position data for a single session\"\"\"\n",
    "        session_drivers = drivers if drivers else session_data['drivers']\n",
    "        session_telemetry = []\n",
    "        \n",
    "        for driver_number in session_drivers:\n",
    "            if driver_number not in session_data['car_data']:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                car_data = session_data['car_data'][driver_number]\n",
    "                pos_data = session_data['pos_data'][driver_number]\n",
    "                \n",
    "                merged = car_data.merge_channels(pos_data, frequency='original')\n",
    "                merged = merged.add_distance().add_differential_distance()\n",
    "                merged['Driver'] = driver_number\n",
    "                \n",
    "                session_telemetry.append(merged)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not merge telemetry for driver {driver_number}: {e}\")\n",
    "        \n",
    "        if session_telemetry:\n",
    "            result = pd.concat(session_telemetry, ignore_index=True)\n",
    "            if 'Date' not in result.columns and 'Time' in result.columns:\n",
    "                result = result.rename(columns={'Time': 'Date'})\n",
    "            elif 'Date' not in result.columns and hasattr(result, 'index') and hasattr(result.index, 'name'):\n",
    "                result = result.reset_index()\n",
    "                if 'index' in result.columns:\n",
    "                    result = result.rename(columns={'index': 'Date'})\n",
    "            return result\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "class TrackStatusLabelEncoder:\n",
    "    \"\"\"Encodes track status labels for safety car prediction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        self.track_status_mapping = {\n",
    "            '1': 'green',\n",
    "            '2': 'yellow',\n",
    "            '4': 'safety_car',\n",
    "            '5': 'red',\n",
    "            '6': 'vsc',\n",
    "            '7': 'vsc_ending'\n",
    "        }\n",
    "    \n",
    "    def fit(self, track_status_data: pd.Series) -> 'TrackStatusLabelEncoder':\n",
    "        mapped_labels = track_status_data.map(self.track_status_mapping).fillna('unknown')\n",
    "        self.label_encoder.fit(mapped_labels)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, track_status_data: pd.Series) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"LabelEncoder must be fitted before transform\")\n",
    "        mapped_labels = track_status_data.map(self.track_status_mapping).fillna('unknown')\n",
    "        return self.label_encoder.transform(mapped_labels)\n",
    "    \n",
    "    def fit_transform(self, track_status_data: pd.Series) -> np.ndarray:\n",
    "        return self.fit(track_status_data).transform(track_status_data)\n",
    "    \n",
    "    def inverse_transform(self, encoded_labels: np.ndarray) -> np.ndarray:\n",
    "        return self.label_encoder.inverse_transform(encoded_labels)\n",
    "    \n",
    "    def get_classes(self) -> np.ndarray:\n",
    "        return self.label_encoder.classes_\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Applies feature engineering to time series data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalization_params = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def handle_missing_values(self, X: np.ndarray, strategy: str = 'forward_fill') -> np.ndarray:\n",
    "        \"\"\"Handle missing values in numeric time series data\"\"\"\n",
    "        if not np.isnan(X).any():\n",
    "            print(\"No missing values detected, skipping imputation\")\n",
    "            return X\n",
    "        \n",
    "        print(f\"Handling missing values with strategy: {strategy}\")\n",
    "        X_filled = X.copy()\n",
    "        \n",
    "        if strategy == 'forward_fill':\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    series = X_filled[i, :, j]\n",
    "                    mask = np.isnan(series)\n",
    "                    if mask.any():\n",
    "                        last_valid = None\n",
    "                        for k in range(len(series)):\n",
    "                            if not np.isnan(series[k]):\n",
    "                                last_valid = series[k]\n",
    "                            elif last_valid is not None:\n",
    "                                series[k] = last_valid\n",
    "                        \n",
    "                        if np.isnan(series[0]):\n",
    "                            valid_indices = np.where(~np.isnan(series))[0]\n",
    "                            if len(valid_indices) > 0:\n",
    "                                fill_value = series[valid_indices[0]]\n",
    "                                for k in range(valid_indices[0]):\n",
    "                                    series[k] = fill_value\n",
    "                        X_filled[i, :, j] = series\n",
    "        elif strategy == 'mean_fill':\n",
    "            for j in range(X.shape[2]):\n",
    "                feature_data = X[:, :, j]\n",
    "                feature_mean = np.nanmean(feature_data)\n",
    "                X_filled[:, :, j] = np.where(np.isnan(feature_data), feature_mean, feature_data)\n",
    "        elif strategy == 'zero_fill':\n",
    "            X_filled = np.where(np.isnan(X_filled), 0, X_filled)\n",
    "            \n",
    "        return X_filled\n",
    "    \n",
    "    def normalize_sequences(self, X: np.ndarray, method: str = 'standard', \n",
    "                           fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Normalize time series sequences\"\"\"\n",
    "        if method == 'standard':\n",
    "            if fit or not self.is_fitted:\n",
    "                means = np.mean(X, axis=(0, 1), keepdims=True)\n",
    "                stds = np.std(X, axis=(0, 1), keepdims=True)\n",
    "                stds = np.where(stds == 0, 1, stds)\n",
    "                self.normalization_params = {'means': means, 'stds': stds}\n",
    "                self.is_fitted = True\n",
    "            \n",
    "            params = self.normalization_params\n",
    "            return (X - params['means']) / params['stds']\n",
    "            \n",
    "        elif method == 'minmax':\n",
    "            if fit or not self.is_fitted:\n",
    "                mins = np.min(X, axis=(0, 1), keepdims=True)\n",
    "                maxs = np.max(X, axis=(0, 1), keepdims=True)\n",
    "                ranges = maxs - mins\n",
    "                ranges = np.where(ranges == 0, 1, ranges)\n",
    "                self.normalization_params = {'mins': mins, 'ranges': ranges}\n",
    "                self.is_fitted = True\n",
    "            \n",
    "            params = self.normalization_params\n",
    "            return (X - params['mins']) / params['ranges']\n",
    "            \n",
    "        elif method == 'per_sequence':\n",
    "            normalized = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                seq = X[i]\n",
    "                seq_mean = np.mean(seq, axis=0, keepdims=True)\n",
    "                seq_std = np.std(seq, axis=0, keepdims=True)\n",
    "                seq_std = np.where(seq_std == 0, 1, seq_std)\n",
    "                normalized[i] = (seq - seq_mean) / seq_std\n",
    "            return normalized\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "\n",
    "class TimeSeriesGenerator:\n",
    "    \"\"\"Generates sliding window time series sequences from telemetry data\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int, step_size: int = 1, \n",
    "                 features: Optional[List[str]] = None,\n",
    "                 prediction_horizon: int = 1,\n",
    "                 handle_non_numeric: str = 'encode'):  # 'encode' or 'drop'\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.handle_non_numeric = handle_non_numeric\n",
    "        self.features = features or [\n",
    "            'Speed', 'RPM', 'nGear', 'Throttle', 'Brake', \n",
    "            'X', 'Y', 'Distance', 'DifferentialDistance'\n",
    "        ]\n",
    "    \n",
    "    def _process_features(self, group_data: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Process features to handle non-numeric data types\n",
    "        Returns numpy array with proper dtype and list of processed feature names\n",
    "        \"\"\"\n",
    "        available_features = [f for f in self.features if f in group_data.columns]\n",
    "        if not available_features:\n",
    "            raise ValueError(f\"No requested features found in data. Available: {list(group_data.columns)}\")\n",
    "        \n",
    "        feature_data = group_data[available_features].copy()\n",
    "        processed_features = []\n",
    "        \n",
    "        for feature in available_features:\n",
    "            col = feature_data[feature]\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(col):\n",
    "                # Already numeric, keep as-is\n",
    "                processed_features.append(feature)\n",
    "            elif pd.api.types.is_bool_dtype(col) or col.dtype == 'bool':\n",
    "                # Boolean - encode as 0/1\n",
    "                if self.handle_non_numeric == 'encode':\n",
    "                    feature_data[feature] = col.astype(int)\n",
    "                    processed_features.append(feature)\n",
    "                    print(f\"Encoded boolean feature '{feature}' as 0/1\")\n",
    "                elif self.handle_non_numeric == 'drop':\n",
    "                    print(f\"Dropping boolean feature '{feature}'\")\n",
    "                    feature_data = feature_data.drop(columns=[feature])\n",
    "            elif col.dtype == 'object':\n",
    "                # Check if it's actually numeric stored as object\n",
    "                try:\n",
    "                    converted = pd.to_numeric(col, errors='coerce')\n",
    "                    if not converted.isna().all():\n",
    "                        feature_data[feature] = converted\n",
    "                        processed_features.append(feature)\n",
    "                        print(f\"Converted object feature '{feature}' to numeric\")\n",
    "                    else:\n",
    "                        # Non-numeric object\n",
    "                        if self.handle_non_numeric == 'encode':\n",
    "                            # Simple label encoding for categorical\n",
    "                            unique_vals = col.unique()\n",
    "                            mapping = {val: i for i, val in enumerate(unique_vals)}\n",
    "                            feature_data[feature] = col.map(mapping)\n",
    "                            processed_features.append(feature)\n",
    "                            print(f\"Label encoded categorical feature '{feature}': {mapping}\")\n",
    "                        elif self.handle_non_numeric == 'drop':\n",
    "                            print(f\"Dropping non-numeric feature '{feature}'\")\n",
    "                            feature_data = feature_data.drop(columns=[feature])\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process feature '{feature}': {e}\")\n",
    "                    if self.handle_non_numeric == 'drop':\n",
    "                        feature_data = feature_data.drop(columns=[feature])\n",
    "            else:\n",
    "                # Other data types\n",
    "                if self.handle_non_numeric == 'drop':\n",
    "                    print(f\"Dropping unsupported feature '{feature}' (dtype: {col.dtype})\")\n",
    "                    feature_data = feature_data.drop(columns=[feature])\n",
    "                else:\n",
    "                    print(f\"Warning: Attempting to convert '{feature}' (dtype: {col.dtype}) to numeric\")\n",
    "                    try:\n",
    "                        feature_data[feature] = pd.to_numeric(col, errors='coerce')\n",
    "                        processed_features.append(feature)\n",
    "                    except:\n",
    "                        feature_data = feature_data.drop(columns=[feature])\n",
    "        \n",
    "        if feature_data.empty:\n",
    "            raise ValueError(\"No valid features remaining after processing\")\n",
    "        \n",
    "        # Convert to numeric numpy array\n",
    "        try:\n",
    "            feature_array = feature_data[processed_features].astype(np.float64).values\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting to float64: {e}\")\n",
    "            print(f\"Data types: {feature_data[processed_features].dtypes}\")\n",
    "            raise\n",
    "        \n",
    "        return feature_array, processed_features\n",
    "    \n",
    "    def generate_sequences(self, telemetry_data: pd.DataFrame, \n",
    "                         group_by: List[str] = None) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:\n",
    "        \"\"\"Generate sliding window sequences with built-in preprocessing\"\"\"\n",
    "        if group_by is None:\n",
    "            group_by = ['SessionId', 'Driver']\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        print(f\"Processing {len(telemetry_data)} total telemetry rows\")\n",
    "        print(f\"Grouping by: {group_by}\")\n",
    "        print(f\"Available columns: {list(telemetry_data.columns)}\")\n",
    "        \n",
    "        group_count = 0\n",
    "        for group_keys, group_data in telemetry_data.groupby(group_by):\n",
    "            group_count += 1\n",
    "            print(f\"\\nProcessing group {group_count}: {group_keys}\")\n",
    "            print(f\"  Group size: {len(group_data)} rows\")\n",
    "            print(f\"  Required minimum rows: {self.window_size + self.prediction_horizon}\")\n",
    "            \n",
    "            try:\n",
    "                group_sequences, group_labels, group_metadata = self._generate_group_sequences(\n",
    "                    group_data, group_keys, group_by\n",
    "                )\n",
    "                print(f\"  Generated {len(group_sequences)} sequences\")\n",
    "                sequences.extend(group_sequences)\n",
    "                labels.extend(group_labels)\n",
    "                metadata.extend(group_metadata)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing group {group_keys}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nTotal sequences generated: {len(sequences)}\")\n",
    "        if not sequences:\n",
    "            print(\"Debugging info:\")\n",
    "            print(f\"  Total groups processed: {group_count}\")\n",
    "            print(f\"  Window size: {self.window_size}\")\n",
    "            print(f\"  Prediction horizon: {self.prediction_horizon}\")\n",
    "            print(f\"  Required features: {self.features}\")\n",
    "            \n",
    "            # Check if TrackStatus column exists\n",
    "            if 'TrackStatus' not in telemetry_data.columns:\n",
    "                print(f\"  ERROR: TrackStatus column missing! Available: {list(telemetry_data.columns)}\")\n",
    "            else:\n",
    "                print(f\"  TrackStatus values: {telemetry_data['TrackStatus'].unique()}\")\n",
    "            \n",
    "            raise ValueError(\"No sequences generated - see debug output above\")\n",
    "        \n",
    "        return np.array(sequences), np.array(labels), metadata\n",
    "    \n",
    "    def _generate_group_sequences(self, group_data: pd.DataFrame, \n",
    "                                group_keys: Tuple, \n",
    "                                group_by: List[str]) -> Tuple[List, List, List]:\n",
    "        \"\"\"Generate sequences for a single group\"\"\"\n",
    "        # Sort by time\n",
    "        if 'Date' not in group_data.columns:\n",
    "            raise ValueError(f\"Date column missing from group data. Available: {list(group_data.columns)}\")\n",
    "        \n",
    "        group_data_sorted = group_data.sort_values('Date').reset_index(drop=True)\n",
    "        print(f\"    Sorted group data: {len(group_data_sorted)} rows\")\n",
    "        \n",
    "        # Process features to handle non-numeric data\n",
    "        try:\n",
    "            feature_array, processed_features = self._process_features(group_data_sorted)\n",
    "            print(f\"    Processed features: {processed_features}\")\n",
    "            print(f\"    Feature array shape: {feature_array.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Feature processing failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        max_start_idx = len(feature_array) - self.window_size - self.prediction_horizon + 1\n",
    "        print(f\"    Max start index: {max_start_idx} (need >= 0 to generate sequences)\")\n",
    "        \n",
    "        if max_start_idx <= 0:\n",
    "            print(f\"    Insufficient data: need {self.window_size + self.prediction_horizon} rows, have {len(feature_array)}\")\n",
    "            return sequences, labels, metadata\n",
    "        \n",
    "        # Check for TrackStatus column\n",
    "        if 'TrackStatus' not in group_data_sorted.columns:\n",
    "            print(f\"    TrackStatus column missing! Available: {list(group_data_sorted.columns)}\")\n",
    "            return sequences, labels, metadata\n",
    "        \n",
    "        sequences_generated = 0\n",
    "        for i in range(0, max_start_idx, self.step_size):\n",
    "            sequence = feature_array[i:i + self.window_size]\n",
    "            \n",
    "            label_idx = i + self.window_size + self.prediction_horizon - 1\n",
    "            if label_idx < len(group_data_sorted):\n",
    "                label = group_data_sorted.iloc[label_idx]['TrackStatus']\n",
    "            else:\n",
    "                print(f\"    Label index {label_idx} out of bounds (max: {len(group_data_sorted)-1})\")\n",
    "                continue\n",
    "            \n",
    "            seq_metadata = {\n",
    "                'start_time': group_data_sorted.iloc[i]['Date'],\n",
    "                'end_time': group_data_sorted.iloc[i + self.window_size - 1]['Date'],\n",
    "                'prediction_time': group_data_sorted.iloc[label_idx]['Date'],\n",
    "                'sequence_length': self.window_size,\n",
    "                'prediction_horizon': self.prediction_horizon,\n",
    "                'features_used': processed_features\n",
    "            }\n",
    "            \n",
    "            for j, key in enumerate(group_by):\n",
    "                seq_metadata[key] = group_keys[j] if isinstance(group_keys, tuple) else group_keys\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "            metadata.append(seq_metadata)\n",
    "            sequences_generated += 1\n",
    "        \n",
    "        print(f\"    Successfully generated {sequences_generated} sequences\")\n",
    "        return sequences, labels, metadata\n",
    "\n",
    "def create_safety_car_dataset(config: DataConfig, \n",
    "                             window_size: int = 100,\n",
    "                             prediction_horizon: int = 10,\n",
    "                             handle_non_numeric: str = 'encode',\n",
    "                             normalization_method: str = 'standard') -> Dict[str, Any]:\n",
    "    \"\"\"Complete ETL pipeline for safety car prediction dataset\"\"\"\n",
    "    \n",
    "    # Step 1: Extract raw data\n",
    "    extractor = RawDataExtractor(config.cache_dir)\n",
    "    sessions_data = [extractor.extract_session(session_config) \n",
    "                    for session_config in config.sessions]\n",
    "    \n",
    "    # Step 2: Aggregate data with track status alignment\n",
    "    aggregator = DataAggregator()\n",
    "    telemetry_data = aggregator.aggregate_telemetry_data(sessions_data, config.drivers)\n",
    "    \n",
    "    if telemetry_data.empty:\n",
    "        raise ValueError(\"No telemetry data extracted\")\n",
    "    \n",
    "    # Step 3: Encode track status labels\n",
    "    label_encoder = TrackStatusLabelEncoder()\n",
    "    if 'TrackStatus' in telemetry_data.columns:\n",
    "        encoded_labels = label_encoder.fit_transform(telemetry_data['TrackStatus'])\n",
    "        telemetry_data['TrackStatusEncoded'] = encoded_labels\n",
    "    else:\n",
    "        raise ValueError(\"No track status data found\")\n",
    "    \n",
    "    # Step 4: Generate time series sequences with built-in preprocessing\n",
    "    ts_generator = TimeSeriesGenerator(\n",
    "        window_size=window_size, \n",
    "        step_size=window_size // 2,\n",
    "        prediction_horizon=prediction_horizon,\n",
    "        handle_non_numeric=handle_non_numeric\n",
    "    )\n",
    "    \n",
    "    X, y, metadata = ts_generator.generate_sequences(telemetry_data)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No sequences generated\")\n",
    "    \n",
    "    print(f\"Generated {len(X)} sequences with shape {X.shape}\")\n",
    "    \n",
    "    # Step 5: Apply feature engineering (missing values + normalization)\n",
    "    engineer = FeatureEngineer()\n",
    "    \n",
    "    # Handle missing values (only if they exist)\n",
    "    X_clean = engineer.handle_missing_values(X, strategy='forward_fill')\n",
    "    \n",
    "    # Normalize sequences\n",
    "    X_normalized = engineer.normalize_sequences(X_clean, method=normalization_method)\n",
    "    \n",
    "    # Encode prediction labels\n",
    "    y_encoded = label_encoder.transform(pd.Series(y))\n",
    "    \n",
    "    # Calculate class distribution\n",
    "    unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "    class_distribution = dict(zip(\n",
    "        label_encoder.inverse_transform(unique), \n",
    "        counts\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        'X': X_normalized,\n",
    "        'y': y_encoded,\n",
    "        'y_raw': y,\n",
    "        'metadata': metadata,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_engineer': engineer,\n",
    "        'raw_telemetry': telemetry_data,\n",
    "        'class_distribution': class_distribution,\n",
    "        'config': {\n",
    "            'window_size': window_size,\n",
    "            'prediction_horizon': prediction_horizon,\n",
    "            'handle_non_numeric': handle_non_numeric,\n",
    "            'normalization_method': normalization_method,\n",
    "            'n_sequences': len(X_normalized),\n",
    "            'n_features': X_normalized.shape[2],\n",
    "            'feature_names': metadata[0]['features_used'] if metadata else []\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "76917871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "F1 Safety Car Prediction ETL Pipeline - Fixed Version\n",
    "\n",
    "Key fixes:\n",
    "1. Proper handling of mixed data types from FastF1\n",
    "2. Feature validation and type checking\n",
    "3. Preprocessing order following aeon principles\n",
    "4. Avoid redundant preprocessing steps\n",
    "5. Configurable target column\n",
    "6. Proper logging support\n",
    "\"\"\"\n",
    "\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "def setup_logger(name: str = 'f1_etl', level: int = logging.INFO, \n",
    "                 enable_debug: bool = False) -> logging.Logger:\n",
    "    \"\"\"Setup logger for the ETL pipeline\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG if enable_debug else level)\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Create console handler\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(logging.DEBUG if enable_debug else level)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "# Default logger\n",
    "logger = setup_logger()\n",
    "\n",
    "@dataclass\n",
    "class SessionConfig:\n",
    "    \"\"\"Configuration for a single F1 session\"\"\"\n",
    "    year: int\n",
    "    race: str\n",
    "    session_type: str\n",
    "\n",
    "@dataclass \n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data processing\"\"\"\n",
    "    sessions: List[SessionConfig]\n",
    "    drivers: Optional[List[str]] = None\n",
    "    telemetry_frequency: Union[str, int] = 'original'\n",
    "    include_weather: bool = True\n",
    "    cache_dir: Optional[str] = None\n",
    "\n",
    "class RawDataExtractor:\n",
    "    \"\"\"Handles extraction of raw fastf1 data\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[str] = None):\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else None\n",
    "        if self.cache_dir:\n",
    "            self.cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def extract_session(self, config: SessionConfig) -> Dict[str, Any]:\n",
    "        \"\"\"Extract all data for a single session\"\"\"\n",
    "        print(f\"Loading session: {config.year} {config.race} {config.session_type}\")\n",
    "        \n",
    "        cache_key = f\"{config.year}_{config.race}_{config.session_type}\".replace(' ', '_')\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\" if self.cache_dir else None\n",
    "        \n",
    "        if cache_file and cache_file.exists():\n",
    "            print(f\"Loading from cache: {cache_file}\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        session = fastf1.get_session(config.year, config.race, config.session_type)\n",
    "        session.load()\n",
    "        \n",
    "        driver_mapping = {}\n",
    "        for driver_number in session.drivers:\n",
    "            driver_info = session.get_driver(driver_number)\n",
    "            driver_mapping[driver_number] = driver_info['Abbreviation']\n",
    "        \n",
    "        session_data = {\n",
    "            'session_info': {\n",
    "                'year': config.year,\n",
    "                'race': config.race,\n",
    "                'session_type': config.session_type,\n",
    "                'event_name': session.event.EventName,\n",
    "                'event_date': session.event.EventDate,\n",
    "                'session_start': session.session_start_time,\n",
    "                't0_date': session.t0_date\n",
    "            },\n",
    "            'laps': session.laps,\n",
    "            'weather': session.weather_data,\n",
    "            'track_status': session.track_status,\n",
    "            'car_data': {},\n",
    "            'pos_data': {},\n",
    "            'drivers': list(session.drivers),\n",
    "            'driver_mapping': driver_mapping\n",
    "        }\n",
    "        \n",
    "        for driver_number in session_data['drivers']:\n",
    "            try:\n",
    "                session_data['car_data'][driver_number] = session.car_data[driver_number]\n",
    "                session_data['pos_data'][driver_number] = session.pos_data[driver_number]\n",
    "            except Exception as e:\n",
    "                abbreviation = driver_mapping.get(driver_number, driver_number)\n",
    "                print(f\"Warning: Could not extract telemetry for driver ({abbreviation}, {driver_number}): {e}\")\n",
    "        \n",
    "        if cache_file:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(session_data, f)\n",
    "        \n",
    "        return session_data\n",
    "\n",
    "class DataAggregator:\n",
    "    \"\"\"Aggregates raw data across multiple sessions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.aggregated_data = defaultdict(list)\n",
    "    \n",
    "    def aggregate_telemetry_data(self, sessions_data: List[Dict[str, Any]], \n",
    "                            drivers: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate telemetry data across sessions with track status alignment\"\"\"\n",
    "        all_telemetry = []\n",
    "        \n",
    "        for session_data in sessions_data:\n",
    "            session_telemetry = self._merge_session_telemetry(session_data, drivers)\n",
    "            \n",
    "            session_telemetry['SessionYear'] = session_data['session_info']['year']\n",
    "            session_telemetry['SessionRace'] = session_data['session_info']['race']\n",
    "            session_telemetry['SessionType'] = session_data['session_info']['session_type']\n",
    "            session_telemetry['SessionId'] = f\"{session_data['session_info']['year']}_{session_data['session_info']['race']}_{session_data['session_info']['session_type']}\"\n",
    "            \n",
    "            track_status = session_data.get('track_status', pd.DataFrame())\n",
    "            t0_date = session_data['session_info']['t0_date']\n",
    "            session_telemetry = self._align_track_status(session_telemetry, track_status, t0_date)\n",
    "            \n",
    "            all_telemetry.append(session_telemetry)\n",
    "        \n",
    "        return pd.concat(all_telemetry, ignore_index=True)\n",
    "\n",
    "    def _align_track_status(self, telemetry: pd.DataFrame, track_status: pd.DataFrame, t0_date) -> pd.DataFrame:\n",
    "        \"\"\"Align track status with telemetry timestamps using forward fill\"\"\"\n",
    "        if track_status is None or track_status.empty or telemetry.empty:\n",
    "            if not telemetry.empty:\n",
    "                telemetry['TrackStatus'] = '1'\n",
    "                telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        if 'Date' not in telemetry.columns:\n",
    "            logger.warning(\"No Date column in telemetry data, skipping track status alignment\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        if 'Time' not in track_status.columns or 'Status' not in track_status.columns:\n",
    "            logger.warning(\"Track status data missing required columns, using default\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "        \n",
    "        try:\n",
    "            track_status_with_date = track_status.copy()\n",
    "            track_status_with_date['Date'] = t0_date + track_status_with_date['Time']\n",
    "            \n",
    "            status_cols = ['Date', 'Status']\n",
    "            if 'Message' in track_status_with_date.columns:\n",
    "                status_cols.append('Message')\n",
    "            \n",
    "            telemetry_with_status = pd.merge_asof(\n",
    "                telemetry.sort_values('Date'),\n",
    "                track_status_with_date[status_cols].sort_values('Date'),\n",
    "                on='Date',\n",
    "                direction='backward'\n",
    "            ).fillna({'Status': '1'})\n",
    "            \n",
    "            if 'Message' not in telemetry_with_status.columns:\n",
    "                telemetry_with_status['Message'] = 'AllClear'\n",
    "            else:\n",
    "                telemetry_with_status['Message'] = telemetry_with_status['Message'].fillna('AllClear')\n",
    "            \n",
    "            telemetry_with_status = telemetry_with_status.rename(columns={\n",
    "                'Status': 'TrackStatus',\n",
    "                'Message': 'TrackStatusMessage'\n",
    "            })\n",
    "            \n",
    "            return telemetry_with_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to align track status: {e}\")\n",
    "            telemetry['TrackStatus'] = '1'\n",
    "            telemetry['TrackStatusMessage'] = 'AllClear'\n",
    "            return telemetry\n",
    "    \n",
    "    def _merge_session_telemetry(self, session_data: Dict[str, Any], \n",
    "                               drivers: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Merge car and position data for a single session\"\"\"\n",
    "        session_drivers = drivers if drivers else session_data['drivers']\n",
    "        session_telemetry = []\n",
    "        \n",
    "        for driver_number in session_drivers:\n",
    "            if driver_number not in session_data['car_data']:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                car_data = session_data['car_data'][driver_number]\n",
    "                pos_data = session_data['pos_data'][driver_number]\n",
    "                \n",
    "                merged = car_data.merge_channels(pos_data, frequency='original')\n",
    "                merged = merged.add_distance().add_differential_distance()\n",
    "                merged['Driver'] = driver_number\n",
    "                \n",
    "                session_telemetry.append(merged)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not merge telemetry for driver {driver_number}: {e}\")\n",
    "        \n",
    "        if session_telemetry:\n",
    "            result = pd.concat(session_telemetry, ignore_index=True)\n",
    "            if 'Date' not in result.columns and 'Time' in result.columns:\n",
    "                result = result.rename(columns={'Time': 'Date'})\n",
    "            elif 'Date' not in result.columns and hasattr(result, 'index') and hasattr(result.index, 'name'):\n",
    "                result = result.reset_index()\n",
    "                if 'index' in result.columns:\n",
    "                    result = result.rename(columns={'index': 'Date'})\n",
    "            return result\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "class TrackStatusLabelEncoder:\n",
    "    \"\"\"Encodes track status labels for safety car prediction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        self.track_status_mapping = {\n",
    "            '1': 'green',\n",
    "            '2': 'yellow',\n",
    "            '4': 'safety_car',\n",
    "            '5': 'red',\n",
    "            '6': 'vsc',\n",
    "            '7': 'vsc_ending'\n",
    "        }\n",
    "    \n",
    "    def fit(self, track_status_data: pd.Series) -> 'TrackStatusLabelEncoder':\n",
    "        mapped_labels = track_status_data.map(self.track_status_mapping).fillna('unknown')\n",
    "        self.label_encoder.fit(mapped_labels)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, track_status_data: pd.Series) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"LabelEncoder must be fitted before transform\")\n",
    "        mapped_labels = track_status_data.map(self.track_status_mapping).fillna('unknown')\n",
    "        return self.label_encoder.transform(mapped_labels)\n",
    "    \n",
    "    def fit_transform(self, track_status_data: pd.Series) -> np.ndarray:\n",
    "        return self.fit(track_status_data).transform(track_status_data)\n",
    "    \n",
    "    def inverse_transform(self, encoded_labels: np.ndarray) -> np.ndarray:\n",
    "        return self.label_encoder.inverse_transform(encoded_labels)\n",
    "    \n",
    "    def get_classes(self) -> np.ndarray:\n",
    "        return self.label_encoder.classes_\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Applies feature engineering to time series data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalization_params = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def handle_missing_values(self, X: np.ndarray, strategy: str = 'forward_fill') -> np.ndarray:\n",
    "        \"\"\"Handle missing values in numeric time series data\"\"\"\n",
    "        if not np.isnan(X).any():\n",
    "            logger.info(\"No missing values detected, skipping imputation\")\n",
    "            return X\n",
    "        \n",
    "        logger.info(f\"Handling missing values with strategy: {strategy}\")\n",
    "        X_filled = X.copy()\n",
    "        \n",
    "        if strategy == 'forward_fill':\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    series = X_filled[i, :, j]\n",
    "                    mask = np.isnan(series)\n",
    "                    if mask.any():\n",
    "                        last_valid = None\n",
    "                        for k in range(len(series)):\n",
    "                            if not np.isnan(series[k]):\n",
    "                                last_valid = series[k]\n",
    "                            elif last_valid is not None:\n",
    "                                series[k] = last_valid\n",
    "                        \n",
    "                        if np.isnan(series[0]):\n",
    "                            valid_indices = np.where(~np.isnan(series))[0]\n",
    "                            if len(valid_indices) > 0:\n",
    "                                fill_value = series[valid_indices[0]]\n",
    "                                for k in range(valid_indices[0]):\n",
    "                                    series[k] = fill_value\n",
    "                        X_filled[i, :, j] = series\n",
    "        elif strategy == 'mean_fill':\n",
    "            for j in range(X.shape[2]):\n",
    "                feature_data = X[:, :, j]\n",
    "                feature_mean = np.nanmean(feature_data)\n",
    "                X_filled[:, :, j] = np.where(np.isnan(feature_data), feature_mean, feature_data)\n",
    "        elif strategy == 'zero_fill':\n",
    "            X_filled = np.where(np.isnan(X_filled), 0, X_filled)\n",
    "            \n",
    "        return X_filled\n",
    "    \n",
    "    def normalize_sequences(self, X: np.ndarray, method: str = 'standard', \n",
    "                           fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Normalize time series sequences\"\"\"\n",
    "        if method == 'standard':\n",
    "            if fit or not self.is_fitted:\n",
    "                means = np.mean(X, axis=(0, 1), keepdims=True)\n",
    "                stds = np.std(X, axis=(0, 1), keepdims=True)\n",
    "                stds = np.where(stds == 0, 1, stds)\n",
    "                self.normalization_params = {'means': means, 'stds': stds}\n",
    "                self.is_fitted = True\n",
    "            \n",
    "            params = self.normalization_params\n",
    "            return (X - params['means']) / params['stds']\n",
    "            \n",
    "        elif method == 'minmax':\n",
    "            if fit or not self.is_fitted:\n",
    "                mins = np.min(X, axis=(0, 1), keepdims=True)\n",
    "                maxs = np.max(X, axis=(0, 1), keepdims=True)\n",
    "                ranges = maxs - mins\n",
    "                ranges = np.where(ranges == 0, 1, ranges)\n",
    "                self.normalization_params = {'mins': mins, 'ranges': ranges}\n",
    "                self.is_fitted = True\n",
    "            \n",
    "            params = self.normalization_params\n",
    "            return (X - params['mins']) / params['ranges']\n",
    "            \n",
    "        elif method == 'per_sequence':\n",
    "            normalized = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                seq = X[i]\n",
    "                seq_mean = np.mean(seq, axis=0, keepdims=True)\n",
    "                seq_std = np.std(seq, axis=0, keepdims=True)\n",
    "                seq_std = np.where(seq_std == 0, 1, seq_std)\n",
    "                normalized[i] = (seq - seq_mean) / seq_std\n",
    "            return normalized\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "\n",
    "class TimeSeriesGenerator:\n",
    "    \"\"\"Generates sliding window time series sequences from telemetry data\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int, step_size: int = 1, \n",
    "                 features: Optional[List[str]] = None,\n",
    "                 prediction_horizon: int = 1,\n",
    "                 handle_non_numeric: str = 'encode',  # 'encode' or 'drop'\n",
    "                 target_column: str = 'TrackStatus'):  # Configurable target column\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.handle_non_numeric = handle_non_numeric\n",
    "        self.target_column = target_column\n",
    "        self.features = features or [\n",
    "            'Speed', 'RPM', 'nGear', 'Throttle', 'Brake', \n",
    "            'X', 'Y', 'Distance', 'DifferentialDistance'\n",
    "        ]\n",
    "    \n",
    "    def _process_features(self, group_data: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Process features to handle non-numeric data types\n",
    "        Returns numpy array with proper dtype and list of processed feature names\n",
    "        \"\"\"\n",
    "        available_features = [f for f in self.features if f in group_data.columns]\n",
    "        if not available_features:\n",
    "            raise ValueError(f\"No requested features found in data. Available: {list(group_data.columns)}\")\n",
    "        \n",
    "        feature_data = group_data[available_features].copy()\n",
    "        processed_features = []\n",
    "        \n",
    "        for feature in available_features:\n",
    "            col = feature_data[feature]\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(col):\n",
    "                # Already numeric, keep as-is\n",
    "                processed_features.append(feature)\n",
    "            elif pd.api.types.is_bool_dtype(col) or col.dtype == 'bool':\n",
    "                # Boolean - encode as 0/1\n",
    "                if self.handle_non_numeric == 'encode':\n",
    "                    feature_data[feature] = col.astype(int)\n",
    "                    processed_features.append(feature)\n",
    "                    logger.debug(f\"Encoded boolean feature '{feature}' as 0/1\")\n",
    "                elif self.handle_non_numeric == 'drop':\n",
    "                    logger.debug(f\"Dropping boolean feature '{feature}'\")\n",
    "                    feature_data = feature_data.drop(columns=[feature])\n",
    "            elif col.dtype == 'object':\n",
    "                # Check if it's actually numeric stored as object\n",
    "                try:\n",
    "                    converted = pd.to_numeric(col, errors='coerce')\n",
    "                    if not converted.isna().all():\n",
    "                        feature_data[feature] = converted\n",
    "                        processed_features.append(feature)\n",
    "                        logger.debug(f\"Converted object feature '{feature}' to numeric\")\n",
    "                    else:\n",
    "                        # Non-numeric object\n",
    "                        if self.handle_non_numeric == 'encode':\n",
    "                            # Simple label encoding for categorical\n",
    "                            unique_vals = col.unique()\n",
    "                            mapping = {val: i for i, val in enumerate(unique_vals)}\n",
    "                            feature_data[feature] = col.map(mapping)\n",
    "                            processed_features.append(feature)\n",
    "                            logger.debug(f\"Label encoded categorical feature '{feature}': {mapping}\")\n",
    "                        elif self.handle_non_numeric == 'drop':\n",
    "                            logger.debug(f\"Dropping non-numeric feature '{feature}'\")\n",
    "                            feature_data = feature_data.drop(columns=[feature])\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not process feature '{feature}': {e}\")\n",
    "                    if self.handle_non_numeric == 'drop':\n",
    "                        feature_data = feature_data.drop(columns=[feature])\n",
    "            else:\n",
    "                # Other data types\n",
    "                if self.handle_non_numeric == 'drop':\n",
    "                    logger.debug(f\"Dropping unsupported feature '{feature}' (dtype: {col.dtype})\")\n",
    "                    feature_data = feature_data.drop(columns=[feature])\n",
    "                else:\n",
    "                    logger.warning(f\"Attempting to convert '{feature}' (dtype: {col.dtype}) to numeric\")\n",
    "                    try:\n",
    "                        feature_data[feature] = pd.to_numeric(col, errors='coerce')\n",
    "                        processed_features.append(feature)\n",
    "                    except:\n",
    "                        feature_data = feature_data.drop(columns=[feature])\n",
    "        \n",
    "        if feature_data.empty:\n",
    "            raise ValueError(\"No valid features remaining after processing\")\n",
    "        \n",
    "        # Convert to numeric numpy array\n",
    "        try:\n",
    "            feature_array = feature_data[processed_features].astype(np.float64).values\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting to float64: {e}\")\n",
    "            logger.error(f\"Data types: {feature_data[processed_features].dtypes}\")\n",
    "            raise\n",
    "        \n",
    "        return feature_array, processed_features\n",
    "    \n",
    "    def generate_sequences(self, telemetry_data: pd.DataFrame, \n",
    "                         group_by: List[str] = None) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:\n",
    "        \"\"\"Generate sliding window sequences with built-in preprocessing\"\"\"\n",
    "        if group_by is None:\n",
    "            group_by = ['SessionId', 'Driver']\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        logger.info(f\"Processing {len(telemetry_data)} total telemetry rows\")\n",
    "        logger.info(f\"Grouping by: {group_by}\")\n",
    "        logger.debug(f\"Available columns: {list(telemetry_data.columns)}\")\n",
    "        \n",
    "        group_count = 0\n",
    "        for group_keys, group_data in telemetry_data.groupby(group_by):\n",
    "            group_count += 1\n",
    "            logger.debug(f\"Processing group {group_count}: {group_keys}\")\n",
    "            logger.debug(f\"  Group size: {len(group_data)} rows\")\n",
    "            logger.debug(f\"  Required minimum rows: {self.window_size + self.prediction_horizon}\")\n",
    "            \n",
    "            try:\n",
    "                group_sequences, group_labels, group_metadata = self._generate_group_sequences(\n",
    "                    group_data, group_keys, group_by\n",
    "                )\n",
    "                logger.debug(f\"  Generated {len(group_sequences)} sequences\")\n",
    "                sequences.extend(group_sequences)\n",
    "                labels.extend(group_labels)\n",
    "                metadata.extend(group_metadata)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing group {group_keys}: {e}\")\n",
    "                logger.debug(\"Full traceback:\", exc_info=True)\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Total sequences generated: {len(sequences)}\")\n",
    "        if not sequences:\n",
    "            logger.error(\"No sequences generated - debugging info:\")\n",
    "            logger.error(f\"  Total groups processed: {group_count}\")\n",
    "            logger.error(f\"  Window size: {self.window_size}\")\n",
    "            logger.error(f\"  Prediction horizon: {self.prediction_horizon}\")\n",
    "            logger.error(f\"  Required features: {self.features}\")\n",
    "            logger.error(f\"  Target column: {self.target_column}\")\n",
    "            \n",
    "            # Check if target column exists\n",
    "            if self.target_column not in telemetry_data.columns:\n",
    "                logger.error(f\"  ERROR: {self.target_column} column missing! Available: {list(telemetry_data.columns)}\")\n",
    "            else:\n",
    "                logger.error(f\"  {self.target_column} values: {telemetry_data[self.target_column].unique()}\")\n",
    "            \n",
    "            raise ValueError(\"No sequences generated - see debug output above\")\n",
    "        \n",
    "        return np.array(sequences), np.array(labels), metadata\n",
    "    \n",
    "    def _generate_group_sequences(self, group_data: pd.DataFrame, \n",
    "                                group_keys: Tuple, \n",
    "                                group_by: List[str]) -> Tuple[List, List, List]:\n",
    "        \"\"\"Generate sequences for a single group\"\"\"\n",
    "        # Sort by time\n",
    "        if 'Date' not in group_data.columns:\n",
    "            raise ValueError(f\"Date column missing from group data. Available: {list(group_data.columns)}\")\n",
    "        \n",
    "        group_data_sorted = group_data.sort_values('Date').reset_index(drop=True)\n",
    "        logger.debug(f\"    Sorted group data: {len(group_data_sorted)} rows\")\n",
    "        \n",
    "        # Process features to handle non-numeric data\n",
    "        try:\n",
    "            feature_array, processed_features = self._process_features(group_data_sorted)\n",
    "            logger.debug(f\"    Processed features: {processed_features}\")\n",
    "            logger.debug(f\"    Feature array shape: {feature_array.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"    Feature processing failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        max_start_idx = len(feature_array) - self.window_size - self.prediction_horizon + 1\n",
    "        logger.debug(f\"    Max start index: {max_start_idx} (need >= 0 to generate sequences)\")\n",
    "        \n",
    "        if max_start_idx <= 0:\n",
    "            logger.debug(f\"    Insufficient data: need {self.window_size + self.prediction_horizon} rows, have {len(feature_array)}\")\n",
    "            return sequences, labels, metadata\n",
    "        \n",
    "        # Check for target column\n",
    "        if self.target_column not in group_data_sorted.columns:\n",
    "            logger.debug(f\"    {self.target_column} column missing! Available: {list(group_data_sorted.columns)}\")\n",
    "            return sequences, labels, metadata\n",
    "        \n",
    "        sequences_generated = 0\n",
    "        for i in range(0, max_start_idx, self.step_size):\n",
    "            sequence = feature_array[i:i + self.window_size]\n",
    "            \n",
    "            label_idx = i + self.window_size + self.prediction_horizon - 1\n",
    "            if label_idx < len(group_data_sorted):\n",
    "                label = group_data_sorted.iloc[label_idx][self.target_column]\n",
    "            else:\n",
    "                logger.debug(f\"    Label index {label_idx} out of bounds (max: {len(group_data_sorted)-1})\")\n",
    "                continue\n",
    "            \n",
    "            seq_metadata = {\n",
    "                'start_time': group_data_sorted.iloc[i]['Date'],\n",
    "                'end_time': group_data_sorted.iloc[i + self.window_size - 1]['Date'],\n",
    "                'prediction_time': group_data_sorted.iloc[label_idx]['Date'],\n",
    "                'sequence_length': self.window_size,\n",
    "                'prediction_horizon': self.prediction_horizon,\n",
    "                'features_used': processed_features,\n",
    "                'target_column': self.target_column\n",
    "            }\n",
    "            \n",
    "            for j, key in enumerate(group_by):\n",
    "                seq_metadata[key] = group_keys[j] if isinstance(group_keys, tuple) else group_keys\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "            metadata.append(seq_metadata)\n",
    "            sequences_generated += 1\n",
    "        \n",
    "        logger.debug(f\"    Successfully generated {sequences_generated} sequences\")\n",
    "        return sequences, labels, metadata\n",
    "\n",
    "def create_safety_car_dataset(config: DataConfig, \n",
    "                             window_size: int = 100,\n",
    "                             prediction_horizon: int = 10,\n",
    "                             handle_non_numeric: str = 'encode',\n",
    "                             normalization_method: str = 'standard',\n",
    "                             target_column: str = 'TrackStatus',\n",
    "                             enable_debug: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"Complete ETL pipeline for safety car prediction dataset\"\"\"\n",
    "    \n",
    "    # Setup logging\n",
    "    global logger\n",
    "    logger = setup_logger(enable_debug=enable_debug)\n",
    "    \n",
    "    # Step 1: Extract raw data\n",
    "    extractor = RawDataExtractor(config.cache_dir)\n",
    "    sessions_data = [extractor.extract_session(session_config) \n",
    "                    for session_config in config.sessions]\n",
    "    \n",
    "    # Step 2: Aggregate data with track status alignment\n",
    "    aggregator = DataAggregator()\n",
    "    telemetry_data = aggregator.aggregate_telemetry_data(sessions_data, config.drivers)\n",
    "    \n",
    "    if telemetry_data.empty:\n",
    "        raise ValueError(\"No telemetry data extracted\")\n",
    "    \n",
    "    # Step 3: Encode track status labels (if using track status)\n",
    "    label_encoder = None\n",
    "    if target_column == 'TrackStatus':\n",
    "        label_encoder = TrackStatusLabelEncoder()\n",
    "        if 'TrackStatus' in telemetry_data.columns:\n",
    "            encoded_labels = label_encoder.fit_transform(telemetry_data['TrackStatus'])\n",
    "            telemetry_data['TrackStatusEncoded'] = encoded_labels\n",
    "        else:\n",
    "            raise ValueError(\"TrackStatus column not found in telemetry data\")\n",
    "    elif target_column not in telemetry_data.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in telemetry data\")\n",
    "    \n",
    "    # Step 4: Generate time series sequences with built-in preprocessing\n",
    "    ts_generator = TimeSeriesGenerator(\n",
    "        window_size=window_size, \n",
    "        step_size=window_size // 2,\n",
    "        prediction_horizon=prediction_horizon,\n",
    "        handle_non_numeric=handle_non_numeric,\n",
    "        target_column=target_column\n",
    "    )\n",
    "    \n",
    "    X, y, metadata = ts_generator.generate_sequences(telemetry_data)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No sequences generated\")\n",
    "    \n",
    "    logger.info(f\"Generated {len(X)} sequences with shape {X.shape}\")\n",
    "    \n",
    "    # Step 5: Apply feature engineering (missing values + normalization)\n",
    "    engineer = FeatureEngineer()\n",
    "    \n",
    "    # Handle missing values (only if they exist)\n",
    "    X_clean = engineer.handle_missing_values(X, strategy='forward_fill')\n",
    "    \n",
    "    # Normalize sequences\n",
    "    X_normalized = engineer.normalize_sequences(X_clean, method=normalization_method)\n",
    "    \n",
    "    # Encode prediction labels if using track status\n",
    "    if label_encoder:\n",
    "        y_encoded = label_encoder.transform(pd.Series(y))\n",
    "    else:\n",
    "        # For non-track status targets, create a simple label encoder\n",
    "        simple_encoder = LabelEncoder()\n",
    "        y_encoded = simple_encoder.fit_transform(y)\n",
    "        label_encoder = simple_encoder\n",
    "    \n",
    "    # Calculate class distribution\n",
    "    unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "    if hasattr(label_encoder, 'inverse_transform'):\n",
    "        try:\n",
    "            class_labels = label_encoder.inverse_transform(unique)\n",
    "        except:\n",
    "            class_labels = unique\n",
    "    else:\n",
    "        class_labels = unique\n",
    "    \n",
    "    class_distribution = dict(zip(class_labels, counts))\n",
    "    \n",
    "    return {\n",
    "        'X': X_normalized,\n",
    "        'y': y_encoded,\n",
    "        'y_raw': y,\n",
    "        'metadata': metadata,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_engineer': engineer,\n",
    "        'raw_telemetry': telemetry_data,\n",
    "        'class_distribution': class_distribution,\n",
    "        'config': {\n",
    "            'window_size': window_size,\n",
    "            'prediction_horizon': prediction_horizon,\n",
    "            'handle_non_numeric': handle_non_numeric,\n",
    "            'normalization_method': normalization_method,\n",
    "            'target_column': target_column,\n",
    "            'n_sequences': len(X_normalized),\n",
    "            'n_features': X_normalized.shape[2],\n",
    "            'feature_names': metadata[0]['features_used'] if metadata else []\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8858442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for São Paulo Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "core        WARNING \tNo lap data for driver 23\n",
      "core        WARNING \tFailed to perform lap accuracy check - all laps marked as inaccurate (driver 23)\n",
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '31', '10', '63', '16', '4', '22', '81', '30', '44', '11', '50', '77', '14', '24', '55', '43', '23', '18', '27']\n"
     ]
    }
   ],
   "source": [
    "session = fastf1.get_session(2024, \"São Paulo Grand Prix\", \"R\")\n",
    "session.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1d0d3b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DriverLabelEncoder at 0x1612b69d0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_label_enc = DriverLabelEncoder()\n",
    "\n",
    "driver_label_enc.fit_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d41d94e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VER': '1',\n",
       " 'OCO': '31',\n",
       " 'GAS': '10',\n",
       " 'RUS': '63',\n",
       " 'LEC': '16',\n",
       " 'NOR': '4',\n",
       " 'TSU': '22',\n",
       " 'PIA': '81',\n",
       " 'LAW': '30',\n",
       " 'HAM': '44',\n",
       " 'PER': '11',\n",
       " 'BEA': '50',\n",
       " 'BOT': '77',\n",
       " 'ALO': '14',\n",
       " 'ZHO': '24',\n",
       " 'SAI': '55',\n",
       " 'COL': '43',\n",
       " 'ALB': '23',\n",
       " 'STR': '18',\n",
       " 'HUL': '27'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_label_enc.driver_to_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "15bc4de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Saudi Arabian Grand Prix - Race [v3.5.3]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session: 2024 Saudi Arabian Grand Prix R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '16', '81', '14', '63', '38', '4', '44', '27', '23', '20', '31', '2', '22', '3', '77', '24', '18', '10']\n"
     ]
    }
   ],
   "source": [
    "sessions = [SessionConfig(2024, 'Saudi Arabian Grand Prix', 'R')]\n",
    "config = DataConfig(sessions=sessions)\n",
    "window_size = 50\n",
    "prediction_horizon = 5\n",
    "\n",
    "# Step 1: Extract raw data\n",
    "extractor = RawDataExtractor(config.cache_dir)\n",
    "sessions_data = [extractor.extract_session(session_config) \n",
    "                    for session_config in config.sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f00ee6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Status</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time</th>\n",
       "      <th>SessionTime</th>\n",
       "      <th>RPM</th>\n",
       "      <th>Speed</th>\n",
       "      <th>nGear</th>\n",
       "      <th>Throttle</th>\n",
       "      <th>Brake</th>\n",
       "      <th>DRS</th>\n",
       "      <th>Distance</th>\n",
       "      <th>DifferentialDistance</th>\n",
       "      <th>Driver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-09 16:04:17.905</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0 days 00:00:02.685000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-09 16:04:18.105</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.200000</td>\n",
       "      <td>0 days 00:00:02.885000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-09 16:04:18.285</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.380000</td>\n",
       "      <td>0 days 00:00:03.065000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-09 16:04:18.485</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.580000</td>\n",
       "      <td>0 days 00:00:03.265000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-09 16:04:18.665</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.760000</td>\n",
       "      <td>0 days 00:00:03.445000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66144</th>\n",
       "      <td>2024-03-09 18:29:24.956</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>-1736.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>car</td>\n",
       "      <td>0 days 02:25:07.051000</td>\n",
       "      <td>0 days 02:25:09.736000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>335321.361613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66145</th>\n",
       "      <td>2024-03-09 18:29:25.071</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>-1736.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 02:25:07.166000</td>\n",
       "      <td>0 days 02:25:09.851000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>335321.361613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66146</th>\n",
       "      <td>2024-03-09 18:29:25.156</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>-1736.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>car</td>\n",
       "      <td>0 days 02:25:07.251000</td>\n",
       "      <td>0 days 02:25:09.936000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>335321.361613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66147</th>\n",
       "      <td>2024-03-09 18:29:26.251</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>-1736.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 02:25:08.346000</td>\n",
       "      <td>0 days 02:25:11.031000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>335321.361613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66148</th>\n",
       "      <td>2024-03-09 18:29:26.276</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>-1736.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>car</td>\n",
       "      <td>0 days 02:25:08.371000</td>\n",
       "      <td>0 days 02:25:11.056000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>335321.361613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66149 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Date   Status       X       Y      Z Source  \\\n",
       "0     2024-03-09 16:04:17.905  OnTrack     0.0     0.0    0.0    pos   \n",
       "1     2024-03-09 16:04:18.105  OnTrack     0.0     0.0    0.0    pos   \n",
       "2     2024-03-09 16:04:18.285  OnTrack     0.0     0.0    0.0    pos   \n",
       "3     2024-03-09 16:04:18.485  OnTrack     0.0     0.0    0.0    pos   \n",
       "4     2024-03-09 16:04:18.665  OnTrack     0.0     0.0    0.0    pos   \n",
       "...                       ...      ...     ...     ...    ...    ...   \n",
       "66144 2024-03-09 18:29:24.956  OnTrack -1736.0  1430.0  117.0    car   \n",
       "66145 2024-03-09 18:29:25.071  OnTrack -1736.0  1430.0  117.0    pos   \n",
       "66146 2024-03-09 18:29:25.156  OnTrack -1736.0  1430.0  117.0    car   \n",
       "66147 2024-03-09 18:29:26.251  OnTrack -1736.0  1430.0  117.0    pos   \n",
       "66148 2024-03-09 18:29:26.276  OnTrack -1736.0  1430.0  117.0    car   \n",
       "\n",
       "                        Time            SessionTime  RPM  Speed  nGear  \\\n",
       "0            0 days 00:00:00 0 days 00:00:02.685000  0.0    0.0      0   \n",
       "1     0 days 00:00:00.200000 0 days 00:00:02.885000  0.0    0.0      0   \n",
       "2     0 days 00:00:00.380000 0 days 00:00:03.065000  0.0    0.0      0   \n",
       "3     0 days 00:00:00.580000 0 days 00:00:03.265000  0.0    0.0      0   \n",
       "4     0 days 00:00:00.760000 0 days 00:00:03.445000  0.0    0.0      0   \n",
       "...                      ...                    ...  ...    ...    ...   \n",
       "66144 0 days 02:25:07.051000 0 days 02:25:09.736000  0.0    0.0      0   \n",
       "66145 0 days 02:25:07.166000 0 days 02:25:09.851000  0.0    0.0      0   \n",
       "66146 0 days 02:25:07.251000 0 days 02:25:09.936000  0.0    0.0      0   \n",
       "66147 0 days 02:25:08.346000 0 days 02:25:11.031000  0.0    0.0      0   \n",
       "66148 0 days 02:25:08.371000 0 days 02:25:11.056000  0.0    0.0      0   \n",
       "\n",
       "       Throttle  Brake  DRS       Distance  DifferentialDistance Driver  \n",
       "0           0.0  False    0       0.000000                   0.0      1  \n",
       "1           0.0  False    0       0.000000                   0.0      1  \n",
       "2           0.0  False    0       0.000000                   0.0      1  \n",
       "3           0.0  False    0       0.000000                   0.0      1  \n",
       "4           0.0  False    0       0.000000                   0.0      1  \n",
       "...         ...    ...  ...            ...                   ...    ...  \n",
       "66144     104.0   True    0  335321.361613                   0.0      1  \n",
       "66145     104.0   True    0  335321.361613                   0.0      1  \n",
       "66146     104.0   True    0  335321.361613                   0.0      1  \n",
       "66147     104.0   True    0  335321.361613                   0.0      1  \n",
       "66148     104.0   True    0  335321.361613                   0.0      1  \n",
       "\n",
       "[66149 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(type(sessions_data))\n",
    "display(len(sessions_data))\n",
    "\n",
    "session_data = sessions_data[0]\n",
    "\n",
    "# display(session_data['session_info'])\n",
    "# display(session_data.keys())\n",
    "# display(session_data['car_data'].keys())\n",
    "# display(session_data['track_status'])\n",
    "\n",
    "session_drivers = session_data['drivers']\n",
    "\n",
    "driver_number = session_drivers[0]\n",
    "\n",
    "car_data = session_data['car_data'][driver_number]\n",
    "pos_data = session_data['pos_data'][driver_number]\n",
    "\n",
    "merged = car_data.merge_channels(pos_data, frequency='original')\n",
    "# display(merged)\n",
    "\n",
    "merged = merged.add_distance().add_differential_distance()\n",
    "# display(merged)\n",
    "\n",
    "merged['Driver'] = driver_number\n",
    "display(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "673cab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1322980, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Status_x</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time</th>\n",
       "      <th>SessionTime</th>\n",
       "      <th>RPM</th>\n",
       "      <th>Speed</th>\n",
       "      <th>...</th>\n",
       "      <th>DRS</th>\n",
       "      <th>Distance</th>\n",
       "      <th>DifferentialDistance</th>\n",
       "      <th>Driver</th>\n",
       "      <th>SessionYear</th>\n",
       "      <th>SessionRace</th>\n",
       "      <th>SessionType</th>\n",
       "      <th>SessionId</th>\n",
       "      <th>Status_y</th>\n",
       "      <th>TrackStatusMessage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-09 16:04:17.905</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0 days 00:00:02.685000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-09 16:04:17.905</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0 days 00:00:02.685000</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-09 16:04:17.905</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0 days 00:00:02.685000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-09 16:04:17.905</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0 days 00:00:02.685000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-09 16:04:17.905</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0 days 00:00:02.685000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date Status_x    X    Y    Z Source   Time  \\\n",
       "0 2024-03-09 16:04:17.905  OnTrack  0.0  0.0  0.0    pos 0 days   \n",
       "1 2024-03-09 16:04:17.905  OnTrack  0.0  0.0  0.0    pos 0 days   \n",
       "2 2024-03-09 16:04:17.905  OnTrack  0.0  0.0  0.0    pos 0 days   \n",
       "3 2024-03-09 16:04:17.905  OnTrack  0.0  0.0  0.0    pos 0 days   \n",
       "4 2024-03-09 16:04:17.905  OnTrack  0.0  0.0  0.0    pos 0 days   \n",
       "\n",
       "             SessionTime     RPM  Speed  ...  DRS  Distance  \\\n",
       "0 0 days 00:00:02.685000     0.0    0.0  ...    0       0.0   \n",
       "1 0 days 00:00:02.685000  5040.0    0.0  ...    1       0.0   \n",
       "2 0 days 00:00:02.685000     0.0    0.0  ...    0       0.0   \n",
       "3 0 days 00:00:02.685000     0.0    0.0  ...    0       0.0   \n",
       "4 0 days 00:00:02.685000     0.0    0.0  ...    0       0.0   \n",
       "\n",
       "   DifferentialDistance  Driver  SessionYear               SessionRace  \\\n",
       "0                   0.0       1         2024  Saudi Arabian Grand Prix   \n",
       "1                   0.0      44         2024  Saudi Arabian Grand Prix   \n",
       "2                   0.0      20         2024  Saudi Arabian Grand Prix   \n",
       "3                   0.0       4         2024  Saudi Arabian Grand Prix   \n",
       "4                   0.0      10         2024  Saudi Arabian Grand Prix   \n",
       "\n",
       "  SessionType                        SessionId Status_y TrackStatusMessage  \n",
       "0           R  2024_Saudi Arabian Grand Prix_R        2             Yellow  \n",
       "1           R  2024_Saudi Arabian Grand Prix_R        2             Yellow  \n",
       "2           R  2024_Saudi Arabian Grand Prix_R        2             Yellow  \n",
       "3           R  2024_Saudi Arabian Grand Prix_R        2             Yellow  \n",
       "4           R  2024_Saudi Arabian Grand Prix_R        2             Yellow  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Aggregate data with track status alignment\n",
    "aggregator = DataAggregator()\n",
    "telemetry_data = aggregator.aggregate_telemetry_data(sessions_data, config.drivers)\n",
    "display(telemetry_data.shape)\n",
    "display(telemetry_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "465acd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4906.035975422429)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-5650.4802962521335)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(21813.26258214537)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(7194.610140325074)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(telemetry_data['Y'].mean())\n",
    "display(telemetry_data['Y'].min())\n",
    "display(telemetry_data['Y'].max())\n",
    "display(telemetry_data['Y'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c039c5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66149, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Status_x</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time</th>\n",
       "      <th>SessionTime</th>\n",
       "      <th>RPM</th>\n",
       "      <th>Speed</th>\n",
       "      <th>...</th>\n",
       "      <th>DRS</th>\n",
       "      <th>Distance</th>\n",
       "      <th>DifferentialDistance</th>\n",
       "      <th>Driver</th>\n",
       "      <th>SessionYear</th>\n",
       "      <th>SessionRace</th>\n",
       "      <th>SessionType</th>\n",
       "      <th>SessionId</th>\n",
       "      <th>Status_y</th>\n",
       "      <th>TrackStatusMessage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-09 16:04:17.905</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0 days 00:00:02.685000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-09 16:04:18.105</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.200000</td>\n",
       "      <td>0 days 00:00:02.885000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-09 16:04:18.285</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.380000</td>\n",
       "      <td>0 days 00:00:03.065000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-09 16:04:18.485</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.580000</td>\n",
       "      <td>0 days 00:00:03.265000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-09 16:04:18.665</td>\n",
       "      <td>OnTrack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0 days 00:00:00.760000</td>\n",
       "      <td>0 days 00:00:03.445000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>Saudi Arabian Grand Prix</td>\n",
       "      <td>R</td>\n",
       "      <td>2024_Saudi Arabian Grand Prix_R</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date Status_x    X    Y    Z Source  \\\n",
       "0 2024-03-09 16:04:17.905  OnTrack  0.0  0.0  0.0    pos   \n",
       "1 2024-03-09 16:04:18.105  OnTrack  0.0  0.0  0.0    pos   \n",
       "2 2024-03-09 16:04:18.285  OnTrack  0.0  0.0  0.0    pos   \n",
       "3 2024-03-09 16:04:18.485  OnTrack  0.0  0.0  0.0    pos   \n",
       "4 2024-03-09 16:04:18.665  OnTrack  0.0  0.0  0.0    pos   \n",
       "\n",
       "                    Time            SessionTime  RPM  Speed  ...  DRS  \\\n",
       "0        0 days 00:00:00 0 days 00:00:02.685000  0.0    0.0  ...    0   \n",
       "1 0 days 00:00:00.200000 0 days 00:00:02.885000  0.0    0.0  ...    0   \n",
       "2 0 days 00:00:00.380000 0 days 00:00:03.065000  0.0    0.0  ...    0   \n",
       "3 0 days 00:00:00.580000 0 days 00:00:03.265000  0.0    0.0  ...    0   \n",
       "4 0 days 00:00:00.760000 0 days 00:00:03.445000  0.0    0.0  ...    0   \n",
       "\n",
       "   Distance  DifferentialDistance  Driver  SessionYear  \\\n",
       "0       0.0                   0.0       1         2024   \n",
       "1       0.0                   0.0       1         2024   \n",
       "2       0.0                   0.0       1         2024   \n",
       "3       0.0                   0.0       1         2024   \n",
       "4       0.0                   0.0       1         2024   \n",
       "\n",
       "                SessionRace SessionType                        SessionId  \\\n",
       "0  Saudi Arabian Grand Prix           R  2024_Saudi Arabian Grand Prix_R   \n",
       "1  Saudi Arabian Grand Prix           R  2024_Saudi Arabian Grand Prix_R   \n",
       "2  Saudi Arabian Grand Prix           R  2024_Saudi Arabian Grand Prix_R   \n",
       "3  Saudi Arabian Grand Prix           R  2024_Saudi Arabian Grand Prix_R   \n",
       "4  Saudi Arabian Grand Prix           R  2024_Saudi Arabian Grand Prix_R   \n",
       "\n",
       "  Status_y TrackStatusMessage  \n",
       "0        2             Yellow  \n",
       "1        2             Yellow  \n",
       "2        2             Yellow  \n",
       "3        2             Yellow  \n",
       "4        2             Yellow  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: just get VERs telemetry\n",
    "driver_number_ver = driver_label_enc.driver_to_number['VER']\n",
    "telemetry_ver = aggregator.aggregate_telemetry_data(sessions_data, [driver_number_ver])\n",
    "display(telemetry_ver.shape)\n",
    "display(telemetry_ver.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c0296f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 days 00:08:45.692000</td>\n",
       "      <td>1</td>\n",
       "      <td>AllClear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 days 01:09:15.917000</td>\n",
       "      <td>2</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 days 01:09:49.924000</td>\n",
       "      <td>4</td>\n",
       "      <td>SCDeployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0 days 01:16:35.527000</td>\n",
       "      <td>1</td>\n",
       "      <td>AllClear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time Status     Message\n",
       "0        0 days 00:00:00      2      Yellow\n",
       "1 0 days 00:08:45.692000      1    AllClear\n",
       "2 0 days 01:09:15.917000      2      Yellow\n",
       "3 0 days 01:09:49.924000      4  SCDeployed\n",
       "4 0 days 01:16:35.527000      1    AllClear"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['2', '1', '4'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Encode labels\n",
    "label_encoder = TrackStatusLabelEncoder()\n",
    "\n",
    "encoded_labels = label_encoder.fit_transform(telemetry_data['TrackStatus'])\n",
    "telemetry_data['TrackStatusEncoded'] = encoded_labels\n",
    "\n",
    "display(session_data['track_status'])\n",
    "display(telemetry_data['TrackStatus'].unique())\n",
    "display(telemetry_data['TrackStatusEncoded'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "59324c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:53:25,474 - f1_etl - INFO - Processing 1322980 total telemetry rows\n",
      "2025-06-24 15:53:25,475 - f1_etl - INFO - Grouping by: ['SessionId', 'Driver']\n",
      "2025-06-24 15:53:26,109 - f1_etl - INFO - Total sequences generated: 0\n",
      "2025-06-24 15:53:26,110 - f1_etl - ERROR - No sequences generated - debugging info:\n",
      "2025-06-24 15:53:26,110 - f1_etl - ERROR -   Total groups processed: 20\n",
      "2025-06-24 15:53:26,110 - f1_etl - ERROR -   Window size: 50\n",
      "2025-06-24 15:53:26,111 - f1_etl - ERROR -   Prediction horizon: 5\n",
      "2025-06-24 15:53:26,111 - f1_etl - ERROR -   Required features: ['Speed', 'RPM', 'nGear', 'Throttle', 'Brake', 'X', 'Y', 'Distance', 'DifferentialDistance']\n",
      "2025-06-24 15:53:26,112 - f1_etl - ERROR -   Target column: TrackStatus\n",
      "2025-06-24 15:53:26,112 - f1_etl - ERROR -   ERROR: TrackStatus column missing! Available: ['Date', 'Status_x', 'X', 'Y', 'Z', 'Source', 'Time', 'SessionTime', 'RPM', 'Speed', 'nGear', 'Throttle', 'Brake', 'DRS', 'Distance', 'DifferentialDistance', 'Driver', 'SessionYear', 'SessionRace', 'SessionType', 'SessionId', 'Status_y', 'TrackStatusMessage', 'TrackStatusEncoded']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No sequences generated - see debug output above",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ts_generator = TimeSeriesGenerator(\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# window_size=window_size, \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# step_size=window_size // 2,  # 50% overlap\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# handle_non_numeric: str = 'encode',\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# normalization_method: str = 'standard'\u001b[39;00m\n\u001b[32m     14\u001b[39m ts_generator = TimeSeriesGenerator(\n\u001b[32m     15\u001b[39m         window_size=window_size, \n\u001b[32m     16\u001b[39m         step_size=window_size // \u001b[32m2\u001b[39m,\n\u001b[32m     17\u001b[39m         prediction_horizon=prediction_horizon,\n\u001b[32m     18\u001b[39m         handle_non_numeric=\u001b[33m'\u001b[39m\u001b[33mencode\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m X, y, metadata = \u001b[43mts_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtelemetry_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 509\u001b[39m, in \u001b[36mTimeSeriesGenerator.generate_sequences\u001b[39m\u001b[34m(self, telemetry_data, group_by)\u001b[39m\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    507\u001b[39m         logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.target_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtelemetry_data[\u001b[38;5;28mself\u001b[39m.target_column].unique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo sequences generated - see debug output above\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(sequences), np.array(labels), metadata\n",
      "\u001b[31mValueError\u001b[39m: No sequences generated - see debug output above"
     ]
    }
   ],
   "source": [
    "# ts_generator = TimeSeriesGenerator(\n",
    "# window_size=window_size, \n",
    "# step_size=window_size // 2,  # 50% overlap\n",
    "# prediction_horizon=prediction_horizon,\n",
    "# padding_strategy='none',\n",
    "# truncation_strategy='none'\n",
    "# )\n",
    "\n",
    "# window_size: int = 100,\n",
    "# prediction_horizon: int = 10,\n",
    "# handle_non_numeric: str = 'encode',\n",
    "# normalization_method: str = 'standard'\n",
    "\n",
    "ts_generator = TimeSeriesGenerator(\n",
    "        window_size=window_size, \n",
    "        step_size=window_size // 2,\n",
    "        prediction_horizon=prediction_horizon,\n",
    "        handle_non_numeric='encode',\n",
    "    )\n",
    "X, y, metadata = ts_generator.generate_sequences(telemetry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "962e1191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52880, 50, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.20000000e+01,  3.54700000e+03,  1.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -3.46525045e+03,  4.84287985e+03,  1.85790968e+04,\n",
       "        2.40000000e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'start_time': Timestamp('2024-03-09 17:00:15.018000'),\n",
       " 'end_time': Timestamp('2024-03-09 17:00:21.231000'),\n",
       " 'prediction_time': Timestamp('2024-03-09 17:00:21.751000'),\n",
       " 'sequence_length': 50,\n",
       " 'prediction_horizon': 5,\n",
       " 'features_used': ['Speed',\n",
       "  'RPM',\n",
       "  'nGear',\n",
       "  'Throttle',\n",
       "  'Brake',\n",
       "  'X',\n",
       "  'Y',\n",
       "  'Distance',\n",
       "  'DifferentialDistance'],\n",
       " 'SessionId': '2024_Saudi Arabian Grand Prix_R',\n",
       " 'Driver': '1'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X.shape)\n",
    "# display(X[1000])\n",
    "display(X[1000][0])\n",
    "# display(y[1000])\n",
    "display(metadata[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20234c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values detected, skipping imputation\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Apply feature engineering (missing values + normalization)\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "# Handle missing values (only if they exist)\n",
    "X_clean = engineer.handle_missing_values(X, strategy='forward_fill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "20cd9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize sequences\n",
    "X_normalized = engineer.normalize_sequences(X_clean, method='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "07ae59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode prediction labels\n",
    "y_encoded = label_encoder.transform(pd.Series(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60ab6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class distribution\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(\n",
    "    label_encoder.inverse_transform(unique), \n",
    "    counts\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "da05d1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([47520,  2500,  2860])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'green': np.int64(47520),\n",
       " 'safety_car': np.int64(2500),\n",
       " 'yellow': np.int64(2860)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(unique)\n",
    "display(counts)\n",
    "display(class_distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
